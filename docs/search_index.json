[["index.html", "Non-linear regression Non-Linear Regression and Optimization Methods in LBA Data Analysis Key Optimization Methods in Non-Linear Regression The Levenberg-Marquardt Algorithm: A Hybrid Approach Why Should We Care About These Optimization Methods? Practical Implementation in R", " Non-linear regression Kyungmin In, GCCL 2025-04-01 Non-Linear Regression and Optimization Methods in LBA Data Analysis Non-linear regression is an essential tool for modeling complex data in fields such as biochemistry, immunology, and diagnostics. In particular, when applying Ligand binding assay (LBA), such as Enzyme-Linked Immunosorbent Assay (ELISA), Electrochemiluminescence Immunoassay (ECLIA) data, the relationship between the concentration of a target analyte and the measured signal is often non-linear due to the intricate biochemical interactions that occur during the assay. While linear regression models are typically used for simple linear relationships, ELISA data and other biological assays often require more sophisticated methods. The non-linear nature of the data necessitates using non-linear regression techniques to accurately model the underlying relationships. However, the challenge with non-linear regression lies in the fact that the relationship between variables cannot be directly expressed as a straight line, requiring iterative numerical methods to find the best-fitting model. Key Optimization Methods in Non-Linear Regression The process of fitting a non-linear model to data involves finding the parameter values that minimize the difference between the predicted and observed data. The optimization methods used for this task can significantly affect the quality of the fit. Two popular techniques for solving non-linear regression problems are gradient descent and the Gauss-Newton method, both of which rely on iterative numerical optimization. Gradient Descent: Gradient descent is a first-order optimization algorithm that adjusts the parameters of the model iteratively to reduce the error (often represented as a cost or loss function). In each step, it moves in the direction opposite to the gradient of the cost function, with the size of the step determined by the learning rate. Although simple and intuitive, gradient descent can be slow, and it may struggle to converge to the global minimum if the cost function is complex or contains many local minima. Gauss-Newton Method: The Gauss-Newton method is an optimization algorithm that is particularly useful for least-squares problems, such as those encountered in non-linear regression. It approximates the Hessian matrix (second-order derivatives) using the Jacobian matrix (first-order derivatives), which simplifies the computation and speeds up convergence compared to gradient descent. The Gauss-Newton method is more efficient than gradient descent when the model is close to the true solution, but it can struggle with highly non-linear models or when the initial guess is far from the optimal solution. The Levenberg-Marquardt Algorithm: A Hybrid Approach To overcome the limitations of those methods, the Levenberg-Marquardt algorithm (often referred to as LM algorithm) combines the advantages of both methods and has become a standard technique for solving non-linear least squares problems, including fitting LBA data. The Levenberg-Marquardt algorithm adjusts the optimization process by blending gradient descent and Gauss-Newton. The idea is to take the best aspects of both methods, making it more robust and capable of handling difficult optimization problem by introducing a damping parameter that controls the balance between the gradient descent and Gauss-Newton steps. When the parameters are far from the optimal solution, the algorithm behaves more like gradient descent (which is stable but slower), and when the parameters are closer to the optimal solution, it behaves more like Gauss-Newton (which is faster but more sensitive to starting points). Mathematically, the Levenberg-Marquardt update rule is given by: \\[ \\theta_{\\text{new}} = \\theta_{\\text{old}} +\\left( J^T W J + \\lambda I \\right)^{-1} J^T Wr \\] Where: \\(\\theta_{old}\\) is the current parameter estimates, \\(J\\) is the Jacobian matrix of the residuals, \\(\\mathbf{r}_i\\), - \\(\\lambda_i\\) is the damping parameter that adjusts the step size, \\(I\\) is the identity matrix, and \\(\\mathbf{r}_i\\) represents the residuals (the difference between the model’s predictions and the observed data). The algorithm iteratively updates the parameters until the residuals are minimized, providing an accurate fit for non-linear models. Why Should We Care About These Optimization Methods? Understanding and applying non-linear regression, along with methods like gradient descent, Gauss-Newton, and Levenberg-Marquardt, is essential for researchers working with complex experimental data. In the case of LBA, the accuracy of the data fit directly impacts the sensitivity and reliability of the assay results. Inaccurate model fitting can lead to misleading conclusions, especially when quantifying low concentrations of analytes. Moreover, the Levenberg-Marquardt algorithm’s robustness makes it a popular choice in many scientific and engineering applications, from pharmacokinetics to machine learning. By mastering these optimization methods, researchers can ensure more accurate, efficient, and reliable data analysis. Practical Implementation in R To provide a hands-on understanding of the Levenberg-Marquardt algorithm and its application to LBA assay data analysis, I will walk through an example using custom R code. This code demonstrates how to implement non-linear regression using the Levenberg-Marquardt method, optimizing the parameters of a five-parameter logistic (5PL) model commonly used for ELISA data. The goal is to offer readers deeper understanding of non-linear regression. "],["author.html", "Author", " Author My name is Kyungmin In, and I am the head of the Research Unit at GCCL (https://eng.gccl.co.kr/).I have worked as a bioanalytical team leader for many years at several companies. My academic background includes a Ph.D. in molecular biology, with additional studies in immunology. During my Ph.D. in Germany, I focused on researching rare genetic diseases. I am particularly interested in automation and reproducible data generation and analysis for bioanalysis, which has led me to explore tools like R. Additionally, I have experience using commercial systems like Watson LIMS (Thermo Fisher Scientific) for efficient data management and analysis, in line with the company’s goals.” "],["the-gradient-descent-method.html", "Chapter 1 The Gradient descent method 1.1 Computation of the chi-squared function 1.2 Computation of the gradient 1.3 The parameter updating", " Chapter 1 The Gradient descent method Let’s assume that we’re working with a non-linear model where \\(\\hat{y} = f(\\alpha)\\), and \\(\\hat{y}\\) is the predicted value based on the model parameters \\(\\alpha\\). In regression, we seek the set of parameters \\(\\alpha\\) that best fit the observed data \\(y_i\\)​. The residuals (differences between the observed values and the model predictions) can be weighted by their associated uncertainties, \\(\\sigma_i\\)​, because data points with smaller uncertainties should contribute more to the fit. To quantify the goodness-of-fit, we use the chi-squared function. It is not only a fitting tool but also the very cost function for non-linear model which defined as the following: \\[ \\chi^2(\\alpha) = \\sum_{i} \\frac{(y_i - \\hat{y}_i(\\alpha))^2}{\\sigma_i^2} \\] where: \\(y_i\\) is the observed value for the \\(i\\)-th data point, \\(\\hat{y}_i(a)\\) is the predicted value for the \\(i\\)-th data point, \\(\\sigma_i^2\\) is the variance (or uncertainty) for the \\(i\\)-th data point, \\(\\alpha\\) is the parameter(s) for the function, The chi-squared function reflects the weighted sum of squared residuals, and minimizing it corresponds to finding the optimal set of parameters \\(\\alpha\\) that best match the data, accounting for the uncertainty in each measurement. To find the optimal parameters we compute the gradient of the chi-squared function with respect to \\(\\alpha\\) and update \\(\\alpha\\) iteratively using gradient descent. This method adjusts \\(\\alpha\\) in the direction of the steepest descent (the negative gradient), progressively improving the fit to the data until the function reaches a minimum. This process allows us to efficiently find the values of \\(\\alpha\\) that minimize the discrepancy between the model and the data while incorporating uncertainties. 1.1 Computation of the chi-squared function It can be simplified as the following: \\[\\chi^2(\\alpha) = \\begin{bmatrix}r_1 &amp; r_2 &amp; \\cdots &amp; r_{n-1} &amp; r_{n} \\end{bmatrix}\\begin{bmatrix} \\frac{1}{\\sigma_i^2} &amp; \\cdots &amp; 0 \\\\\\vdots &amp; \\ddots &amp; \\vdots \\\\0 &amp; \\cdots &amp; \\frac{1}{\\sigma_n^2}\\end{bmatrix}\\begin{bmatrix} r_1 \\\\ r_2 \\\\ \\vdots \\\\ r_{n-1} \\\\ r_{n} \\end{bmatrix} \\] \\(r_{i}\\) is the residual for the \\(i\\)-th data point that is \\(y-\\hat y_i(\\alpha)\\) and thus its squares is \\(r_i^2\\) . \\(\\frac{1}{\\sigma_i^2}\\) is the weighted residuals of each data point and also the diagonal elements being the inverse of the variances \\(\\sigma_i^2\\), expressed as \\(W = \\text{diag} \\left( \\frac{1}{\\sigma_1^2}, \\frac{1}{\\sigma_2^2}, \\dots, \\frac{1}{\\sigma_n^2} \\right)\\) or \\[\\begin{bmatrix} \\frac{1}{\\sigma_i^2} &amp; \\cdots &amp; 0 \\\\\\vdots &amp; \\ddots &amp; \\vdots \\\\0 &amp; \\cdots &amp; \\frac{1}{\\sigma_n^2}\\end{bmatrix} \\] So \\(\\chi^2(\\alpha) = \\sum_{i} \\frac{(y_i - \\hat{y}_i(\\alpha))^2}{\\sigma_i^2}\\) can be rewritten simply as \\(\\chi^2(\\alpha) = \\sum_{i}{r_i^2}W\\). The right section \\(\\sum_ir_i^2W\\) can also be rewritten simply as \\(r^TWr\\) since \\(r\\) is a column vector of each residuals: \\[ \\begin{bmatrix} r_1 \\\\ r_2 \\\\ \\vdots \\\\ r_{n-1} \\\\ r_{n} \\end{bmatrix} \\] And the \\(r^T\\) is a transposed vector of the residual vector thus a row vector: \\[ \\begin{bmatrix}r_1 &amp; r_2 &amp; \\cdots &amp; r_{n-1} &amp; r_{n} \\end{bmatrix}\\] And so it can be finally rewritten as \\(\\chi^2(\\alpha) = r^TWr\\) and therefore be: \\[ \\chi^2(\\alpha) = \\begin{bmatrix}r_1 &amp; r_2 &amp; \\cdots &amp; r_{n-1} &amp; r_{n} \\end{bmatrix}\\begin{bmatrix} \\frac{1}{\\sigma_i^2} &amp; \\cdots &amp; 0 \\\\\\vdots &amp; \\ddots &amp; \\vdots \\\\0 &amp; \\cdots &amp; \\frac{1}{\\sigma_n^2}\\end{bmatrix}\\begin{bmatrix} r_1 \\\\ r_2 \\\\ \\vdots \\\\ r_{n-1} \\\\ r_{n} \\end{bmatrix} \\] And that will return a scalar (single value) that the sum of weighted squared residuals. 1.2 Computation of the gradient It is known that the gradient of \\(\\chi^2(a)\\) with respect to \\(a\\) is: \\[ \\frac{\\partial \\chi^2(\\alpha)}{\\partial \\alpha} = -2(y-\\hat y)^TWJ \\] and is the direction in which we want to move \\(a\\) to minimize \\(\\chi^2(a)\\). Let’s beak down to see how we can get the gradient. We can rewrite the chi squared function as: \\[ \\chi^2(\\alpha) = r^TWr = (y-\\hat y(\\alpha))^TW(y-\\hat y(\\alpha)) \\] The far right terms can be expanded as: \\[ (y^TWy-y^TW\\hat y(\\alpha)-\\hat y^T(\\alpha)Wy+\\hat y^T(\\alpha)W\\hat y(\\alpha) \\] The second and third therms will result in the same scalar value and therefore be expressed as: \\[ -2y^TW\\hat y(\\alpha) \\] Combining all: \\[ \\chi^2(a) = y^T W y - 2 y^T W \\hat{y}(\\alpha) + \\hat{y}^T(\\alpha) W \\hat{y}(\\alpha) \\] Or simply: \\[ \\chi^2(a) = y^T W y - 2 y^T W \\hat{y} + \\hat{y}^T W \\hat{y} \\] The gradient of \\(\\chi^2(a)\\) with respect to \\(a\\) is the partial derivation of that with respect to \\(\\alpha\\) so the first section is: \\[\\frac{\\partial y^T W y}{\\partial \\alpha} = 0\\] The second section: \\[ \\frac{\\partial }{\\partial \\alpha}(-2 y^T W \\hat{y}) = -2 y^T W\\frac{\\partial \\hat y }{\\partial \\alpha} \\] The third section: \\[ \\frac{\\partial }{\\partial \\alpha}(\\hat{y}^T W \\hat{y}) = -2 \\hat y^T W\\frac{\\partial \\hat y}{\\partial \\alpha} \\] Thus, \\[ \\frac{\\partial \\chi^2(\\alpha)}{\\partial \\alpha} = 0-2 y^T W\\frac{\\partial \\hat y} {\\partial \\alpha} -2 \\hat y^T W\\frac{\\partial \\hat y}{\\partial \\alpha}\\] \\[ \\frac{\\partial \\chi^2(\\alpha)}{\\partial \\alpha} = -2(y- \\hat y)^TW\\frac{\\partial \\hat y}{\\partial \\alpha} \\] And \\(\\frac{\\partial \\hat y}{\\partial \\alpha}\\) is an Jacobian matrix \\(J\\) and therefore be: \\[ \\frac{\\partial \\chi^2(\\alpha)}{\\partial \\alpha} = -2(y- \\hat y)^TWJ\\] \\(-2(y-\\hat y)^TWJ\\) is also known as \\(-2J^TW(y-\\hat y)\\) because those two will returns the same output, only changing dimension of the output, from a 1 X n row vector to a n X 1 column vector. The equation above can be differently expressed as: \\[ \\nabla_{\\alpha} \\chi^2(\\alpha) = \\sum_{i=1}^n\\frac{2(y_i-f(\\alpha))(-\\nabla_\\alpha f(\\alpha))}{\\theta^2_i} \\] 1.3 The parameter updating We start with an initial guess for the parameters, say \\(\\alpha_{\\text{old}}\\) or \\(\\theta_{\\text{old}}\\), and we want to update them to get closer to the optimal solution. The idea is to move in the direction that reduces the cost function (the “hill” we’re trying to go down). Here’s how the update rule works: Start with the initial parameter: \\(\\alpha_{\\text{old}}\\) (or \\(\\theta_{\\text{old}}\\)) is our starting guess for the parameters. This is usually a random guess, but it doesn’t have to be perfect at the beginning (but always better to start with the best initial parameters). Compute the gradient: The gradient tells you the direction to move. It’s essentially the derivative of the cost function with respect to the parameter. In simpler terms, it’s like a slope of the hill that tells us whether we’re going uphill or downhill and how steep the slope is. Mathematically, the gradient of the cost function \\(\\chi^2\\) with respect to the parameters \\(\\alpha\\) or \\(\\theta\\) is: \\[ \\nabla_{\\alpha} \\chi^2(\\alpha) \\quad \\text{or} \\quad \\nabla_\\theta \\chi^2(\\theta) \\] This gradient is a vector that points in the direction of maximum increase in the cost function. Since we want to decrease the cost, we move in the opposite direction (hence the negative sign). Update the parameters: The rule for updating the parameters is: \\[ \\alpha_{\\text{new}} = \\alpha_{\\text{old}} - \\eta \\nabla_{\\alpha} \\chi^2(\\alpha) \\] or equivalently, \\[ \\theta_{\\text{new}} = \\theta_{\\text{old}} - \\eta \\nabla_\\theta \\chi^2(\\theta) \\] Here: \\(\\eta\\) is the learning rate (a scalar), which controls how large the update step is. If \\(\\eta\\) is too small, the steps are too tiny, and the optimization may take too long to converge. If \\(\\eta\\) is too large, the steps might overshoot, causing the optimization to miss the optimal solution. \\(\\nabla_{\\alpha} \\chi^2(\\alpha)\\) is the gradient of the cost function with respect to the parameters, which tells us the direction to move in. A More Specific Example with \\(J^T W (y - \\hat{y})\\) (I prefer to use this term. Please refer to the note below): \\[ \\nabla_\\theta \\chi^2(\\theta) = -2 J^T W (y - \\hat{y}) \\] Where: \\(J\\) is the Jacobian matrix of the model (it contains the partial derivatives of the model output with respect to the parameters). \\(W\\) is a diagonal matrix with the weights \\(\\frac{1}{\\sigma_i^2}\\) on the diagonal (these weights are used to account for uncertainties in the data). \\(y\\) is the observed data, and \\(\\hat{y}\\) is the predicted model output. So the update rule becomes: \\[ \\theta_{\\text{new}} = \\theta_{\\text{old}} + 2 J^T W (y - \\hat{y}) \\] Note: The positive sign here is because the gradient we calculated has a negative sign, and by flipping it, we ensure that we’re moving in the direction that reduces the cost. Introducing the Learning Rate: As mentioned before, we typically introduce the learning rate \\(\\eta\\) to control the size of each step. So, the update rule becomes: \\[ \\theta_{\\text{new}} = \\theta_{\\text{old}} + \\eta J^T W (y - \\hat{y}) \\] Where: \\(\\eta\\) is the learning rate, and it controls how much we change \\(\\theta_{\\text{old}}\\) in each step. To summarize, we can update the parameter as: \\[ \\theta_{\\text{new}} = \\theta_{\\text{old}} + h_{\\text{gd}} \\] where, \\[ h_{\\text{gd}} = \\eta J^TW(y-\\hat y) \\] "],["the-gauss-newton-method.html", "Chapter 2 The Gauss-Newton Method 2.1 Computation of the chi-squared function 2.2 Computation of the gradient 2.3 The parameter updating 2.4 Summary and Comparison to Gradient Descent", " Chapter 2 The Gauss-Newton Method The Gauss-Newton method is a second-order optimization technique used in non-linear regression to minimize the chi-squared cost function. It is especially useful for non-linear least squares problems where the model’s prediction depends on parameters in a nonlinear way. The key here is that the Gauss-Newton method assumes the cost function behaves approximately linearly around the current parameter \\(\\alpha\\). This means we can approximate the change in the cost function \\(\\chi^2\\) when \\(\\alpha\\) is perturbed by a small amount \\(h\\). Gradient descent, on the other hand, is a first-order optimization method that uses only the gradient (the first derivative) of the cost function to update the parameters. 2.1 Computation of the chi-squared function We can approximate the chi-squared function for a small change in \\(\\alpha\\): \\[ \\chi^2(\\alpha + h) = \\chi^2(\\alpha) + \\frac{\\partial \\chi^2}{\\partial \\alpha} h \\] This is a first-order Taylor expansion of the cost function, assuming that the change in \\(\\alpha\\) is small enough that the higher-order terms are negligible. The term \\(\\frac{\\partial \\chi^2}{\\partial \\alpha}\\) represents the gradient of the chi-squared function with respect to \\(\\alpha\\), which tells us how the cost changes as we move in the direction of \\(h\\). Let’s start with the chi-squared function: \\[ \\chi^2(\\alpha) = y^T W y - 2 y^T W \\hat{y}(\\alpha) + \\hat{y}(\\alpha)^T W \\hat{y}(\\alpha) \\] Where: - \\(y\\) is the observed data, - \\(\\hat{y}(\\alpha)\\) is the predicted value from the model for the current parameter \\(\\alpha\\), - \\(W\\) is a weight matrix. We assume that the chi-squared function around the current parameter \\(\\alpha\\) is linear and that can be expressed as: \\[ \\chi^2(\\alpha +h)\\approx\\chi^2(\\alpha) + \\frac{\\partial \\chi^2}{\\partial \\alpha}h \\] This is a first-order Taylor expansion of the cost function, assuming that the change in \\(\\alpha\\) is small enough that the higher-order terms are negligible. The term \\(\\frac{\\partial \\chi^2}{\\partial \\alpha}\\) represents the gradient of the chi-squared function with respect to \\(\\alpha\\), which tells us how the cost changes as we move in the direction of \\(h\\). Now, we can substitute the expression for \\(\\hat{y}(\\alpha + h)\\) into the cost function. Using the first-order approximation for \\(\\hat{y}(\\alpha + h)\\), which is: \\[ \\hat{y}(\\alpha + h)\\approx \\hat{y}(\\alpha) + J h \\] Where \\(J\\) is the Jacobian matrix, which contains the partial derivatives of the model predictions with respect to the parameters \\(\\alpha\\), we substitute this into the expanded chi-squared function: \\[ \\chi^2(\\alpha + h) \\approx y^T W y - 2 y^T W (\\hat{y}(\\alpha) + J h) + (\\hat{y}(\\alpha) + J h)^T W (\\hat{y}(\\alpha) + J h) \\] This gives the following terms: The first term, \\(y^T W y\\), is constant and doesn’t change with \\(\\alpha\\). The second term expands as: \\[ -2 y^T W (\\hat{y}(\\alpha) + J h) = -2 y^T W \\hat{y}(\\alpha) - 2 y^T W J h \\] The third term is: \\[ (\\hat{y}(\\alpha) + J h)^T W (\\hat{y}(\\alpha) + J h) = \\hat{y}(\\alpha)^T W \\hat{y}(\\alpha) + \\hat{y}(\\alpha)^T W J h + h^T J^T W \\hat{y}(\\alpha) + h^T J^T W J h \\] We can simplify the expression further since: The terms \\(y^T W y\\) and \\(\\hat{y}(\\alpha)^T W \\hat{y}(\\alpha)\\) remain unchanged. The cross terms \\(\\hat{y}(\\alpha)^T W J h\\) and \\(h^T J^T W \\hat{y}(\\alpha)\\) are equal, so they combine into: \\[ 2 \\hat{y}(\\alpha)^T W J h \\] Thus, the simplified expression for \\(\\chi^2(\\alpha + h)\\) is: \\[ \\chi^2(\\alpha + h) \\approx y^T W y - 2 y^T W \\hat{y}(\\alpha) - 2 y^T W J h + \\hat{y}(\\alpha)^T W \\hat{y}(\\alpha) + 2 \\hat{y}(\\alpha)^T W J h + h^T J^T W J h \\] This is the expanded chi-squared function for a small perturbation \\(h\\). 2.2 Computation of the gradient Let’s move on to the next step where we take the derivative of the expanded form of \\(\\chi^2(\\alpha + h)\\) with respect to \\(h\\). We have already expanded \\(\\chi^2(\\alpha + h)\\) as: \\[ \\chi^2(\\alpha + h) \\approx y^T W y + \\hat{y}(\\alpha)^T W \\hat{y}(\\alpha) - 2 y^T W \\hat{y}(\\alpha) - 2 (y - \\hat{y}(\\alpha))^T W J h + h^T J^T W J h \\] so let’s compute the derivative of \\(\\chi^2(\\alpha + h)\\) with respect to the update vector \\(h\\). The first section \\(y^T W y\\), its derivative is 0. The second section \\(\\hat{y}(\\alpha)^T W \\hat{y}(\\alpha)\\), its derivative is also 0. The third section -\\(2 y^T W \\hat{y}(\\alpha)\\) is also 0. The fourth section -\\(2 (y - \\hat{y}(\\alpha))^T W J h\\): \\[ \\frac{\\partial}{\\partial h} \\left( -2 (y - \\hat{y}(\\alpha))^T W J h \\right) = -2 W^T J (y - \\hat{y}(\\alpha)) \\] For the last section \\(h^T J^T W J h\\): \\[ \\frac{\\partial}{\\partial h} (h^T J^T W J h) = 2 J^T W J h \\] Combining all the terms, we can get the derivative of \\(\\chi^2(\\alpha + h)\\) with respect to \\(h\\): \\[ \\frac{\\partial \\chi^2(\\alpha + h)}{\\partial h} \\approx -2 W^T J (y - \\hat{y}(\\alpha)) + 2 J^T W J h \\] It is also commonly known as: \\[ \\frac{\\partial \\chi^2(\\alpha + h)}{\\partial h} \\approx -2 (y - \\hat{y}(\\alpha))^T W J + 2 h^T J^T W J \\] where the \\(J^TWJ\\) is also known as the approximation of Hessian matrix. 2.3 The parameter updating In the previous steps, we derived the gradient of the cost function \\(\\chi^2(\\alpha + h)\\) with respect to the update direction \\(h\\). This gradient provides insight into how the cost function changes as the parameters \\(\\alpha\\) are adjusted by a small amount \\(h\\). Here’s the expression for the gradient: \\[ \\frac{\\partial \\chi^2(\\alpha + h)}{\\partial h} \\approx -2 \\left( (y - \\hat{y}(\\alpha))^T W J + 2 h^T J^T W J \\right) \\] The goal is to find the update direction \\(h_{\\text{gn}}\\) that minimizes the cost function. To do this, we set this derivative equal to zero: \\[ 0 = -2 \\left( (y - \\hat{y}(\\alpha))^T W J + 2 h^T J^T W J \\right) \\] This simplifies to: \\[ h^T J^T W J = (y - \\hat{y}(\\alpha))^T W J \\] Taking the transpose of both sides: \\[ \\left[ h^T J^T W J \\right]^T = \\left[ (y - \\hat{y}(\\alpha))^T W J \\right]^T \\] Which simplifies further to: \\[ J^T W J h = J^T W (y - \\hat{y}(\\alpha)) \\] And finally, we can solve for the update direction \\(h_{\\text{gn}}\\): \\[ h_{\\text{gn}} = \\left( J^T W J \\right)^{-1} J^T W (y - \\hat{y}(\\alpha)) \\] This gives the update direction \\(h_{\\text{gn}}\\), which tells us how to adjust the parameters to reduce the cost. To update the parameters, we simply add this update direction to the current parameters: \\[ \\theta_{\\text{new}} = \\theta_{\\text{old}} + h_{\\text{gn}} \\] where \\(h_{\\text{gn}}\\) is the update direction computed as: \\[ h_{\\text{gn}} = \\left( J^T W J \\right)^{-1} J^T W (y - \\hat{y}(\\alpha)) \\] 2.4 Summary and Comparison to Gradient Descent In this section, we used the Gauss-Newton method to update the parameters. Here’s how it works compared to gradient descent: Initial Guess: We start with an initial guess for the parameters, \\(\\theta_{\\text{old}}\\), which might be a random value but should be reasonable enough to start the optimization process. Compute the Gradient: The gradient (or in this case, the first derivative) of the cost function tells us the direction to move in. It gives us a sense of the slope of the “hill” we’re trying to descend. For gradient descent, the update rule would be: \\[ \\theta_{\\text{new}} = \\theta_{\\text{old}} - \\eta \\nabla_{\\theta} \\chi^2(\\theta) \\] where \\(\\eta\\) is the learning rate (controlling the size of the update step) and \\(\\nabla_{\\theta} \\chi^2(\\theta)\\) is the gradient of the cost function with respect to the parameters. The Gauss-Newton method also uses a gradient-like update, but it takes into account second-order information (through the Jacobian matrix \\(J\\)) to adjust the parameters more effectively, particularly when dealing with nonlinear models. The Gauss-Newton update rule is: \\[ \\theta_{\\text{new}} = \\theta_{\\text{old}} + \\left( J^T W J \\right)^{-1} J^T W (y - \\hat{y}(\\alpha)) \\] Learning Rate: In gradient descent, we often introduce a learning rate \\(\\eta\\) to control the size of the updates. In Gauss-Newton, the update is determined by the matrix inversion, so we don’t need a separate learning rate, though it’s still possible to introduce one if necessary for stability. For gradient descent: \\[ \\theta_{\\text{new}} = \\theta_{\\text{old}} + \\eta J^T W (y - \\hat{y}(\\alpha)) \\] Second-order Information: The key difference is that the Gauss-Newton method uses second-order information (via the Jacobian matrix \\(J\\) and the Hessian approximation \\(J^T W J\\)) to better model how the cost function behaves near the current parameter values. This typically results in more efficient updates compared to gradient descent, especially for nonlinear models. In summary, both methods aim to update the parameters \\(\\theta\\) to minimize the cost function \\(\\chi^2(\\alpha)\\), but the Gauss-Newton method incorporates second-order information (through the Jacobian and its inverse), which allows for more accurate and faster convergence, especially when dealing with nonlinear regression models. "],["levenberg-marquardt-lm-algorithm.html", "Chapter 3 Levenberg-Marquardt (LM) algorithm 3.1 The Damping Factor \\(\\lambda\\) 3.2 Advantages of the LM Algorithm", " Chapter 3 Levenberg-Marquardt (LM) algorithm The Levenberg-Marquardt (LM) algorithm is a combination of two optimization techniques: the Gauss-Newton method and the gradient descent method. It is designed to minimize a non-linear least squares problem, often encountered in regression and curve fitting. The Gauss-Newton method is efficient for problems where the residuals are small and the function can be approximated by a quadratic model. However, the Gauss-Newton method can perform poorly when the function is highly non-linear, or if the initial guess for the parameters is far from the optimal solution. The gradient descent method is more robust and works well even with large non-linearities, but it may require many iterations to converge, especially when the gradient is very small. The LM algorithm smoothly interpolates between the two methods. When the current solution is far from the optimal, it behaves more like gradient descent. As the solution improves and the model becomes more linear, the algorithm shifts towards the Gauss-Newton method. This hybrid approach provides the stability of gradient descent and the efficiency of Gauss-Newton. The LM algorithm can be summarized in the following steps: Initialization: Choose an initial guess \\(\\theta_{\\text{old}}\\) and set the damping factor \\(\\lambda_0\\) to a small positive value. Iteration: Repeat the following steps until convergence: Compute the Jacobian matrix \\(J\\) and the residual vector \\(r = y - \\hat{y}(\\alpha)\\). Compute the Gauss-Newton step with the damping factor: \\[ h_{\\text{lm}} = \\left( J^T W J + \\lambda I \\right)^{-1} J^T Wr \\] Update the parameter vector: \\[ \\theta_{\\text{new}} = \\theta_{\\text{old}} + h_{\\text{lm}} \\] If the new cost function is smaller than the previous one, decrease \\(\\lambda\\); otherwise, increase \\(\\lambda\\). Convergence: The algorithm stops when the change in the cost function or the parameter vector is sufficiently small. Marquardt suggested later to modify the algorithm : \\[ \\theta_{\\text{new}} = \\theta_{\\text{old}} + \\left( J^T W J + \\lambda \\ diag(J^TWJ) \\right)^{-1} J^T W r \\] 3.1 The Damping Factor \\(\\lambda\\) The damping factor \\(\\lambda\\) adjusts the size of the update step. If the current solution is far from the optimum, a larger \\(\\lambda\\) will force the algorithm to behave more like gradient descent, ensuring a more stable update. Conversely, as the solution improves and the model becomes linear, \\(\\lambda\\) is reduced, and the algorithm behaves more like Gauss-Newton, which converges faster. The update rule for \\(\\lambda\\) is typically based on the change in the cost function between iterations: If the new cost function \\(\\chi^2(\\alpha_{\\text{new}})\\) is smaller than the previous cost function \\(\\chi^2(\\alpha_{\\text{old}})\\), then reduce \\(\\lambda\\) to speed up convergence. If the new cost function \\(\\chi^2(\\alpha_{\\text{new}})\\) is larger than the previous one, increase \\(\\lambda\\) to make the algorithm behave more like gradient descent. 3.2 Advantages of the LM Algorithm Efficiency: The LM algorithm is more efficient than pure gradient descent when the problem is approximately linear, and more robust than Gauss-Newton for non-linear problems. Stability: By adjusting the damping factor, the LM algorithm can avoid large updates that could cause divergence, which is a common problem in Gauss-Newton. Versatility: The LM algorithm is widely used in various applications, including curve fitting, machine learning, and computer vision. "],["applying-the-lm-algorithm-to-a-damped-oscillation-model.html", "Chapter 4 Applying the LM Algorithm to a Damped Oscillation Model 4.1 Example Data 4.2 Data fitting using nlsLM 4.3 How the Algorithm Works 4.4 Custom Implementation of the Algorithm 4.5 Convergence Criteria of the Algorithm 4.6 Code Example", " Chapter 4 Applying the LM Algorithm to a Damped Oscillation Model In this chapter, we’ll walk through how to use the Levenberg-Marquardt (LM) algorithm to fit a damped oscillation model to a set of data. This type of model is often used to represent phenomena like decaying waves or oscillations, where the amplitude decreases over time. The general form of the equation for a damped oscillation is: \\[ y = A \\cdot e^{(-\\lambda \\cdot x)} \\cdot \\cos(\\omega \\cdot x + \\Phi) + C \\] Where: \\(A\\) is the amplitude (parameter 1, \\(\\omega_1\\)) \\(\\lambda\\) is the decay rate (parameter 2, \\(\\omega_2\\)) \\(\\omega\\) is the oscillation frequency (parameter 3, \\(\\omega_3\\)) \\(\\Phi\\) is the phase shift (parameter 4, \\(\\omega_4\\)) \\(C\\) is the vertical offset (parameter 5, \\(\\omega_5\\)) We need to estimate these 5 parameters based on the given data. Let’s see how to do this using the LM algorithm, which helps optimize these parameters. The following libraries are necessary for running R codes: library(here) library(tidyverse) library(gganimate) library(minpack.lm) library(numDeriv) library(MASS) library(plotly) library(gridExtra) library(grid) 4.1 Example Data Here’s the data we’ll be working with. x &lt;- c(0.0, 0.69, 1.38, 2.07, 2.76, 3.45, 4.14, 4.83, 5.52, 6.21, 6.9, 7.59, 8.28, 8.97, 9.66, 10.34, 11.03, 11.72, 12.41, 13.1, 13.79, 14.48, 15.17, 15.86, 16.55, 17.24, 17.93, 18.62, 19.31, 20.0) To create the corresponding \\(y\\) values, we’ll use the formula mentioned earlier with certain parameters. f &lt;- function(w0){ return(w0[1] * exp(-w0[2] * x) * cos(w0[3] * x + w0[4]) + w0[5]) } w &lt;- c(1.9992035, 0.1002633, 1.2568461, 0.4967924, 0.9992937) # Real parameters y &lt;- round(f(w), 2) df_temp &lt;- data.frame(x,y) Now, let’s assume we don’t know these exact parameters but we have some initial guesses for them. For example: w0 &lt;- c(3, 0.1, 2*pi/5, 0.5, 1) # Incorrect initial guess This initial guess will give us a poor fit to the data, as we can see below in the graph: d1 &lt;- ggplot(df_temp, aes(x, y)) + geom_point() + stat_function(fun = function(x) w0[1] * exp(-w0[2] * x) * cos(w0[3] * x + w0[4]) + w0[5], col = &quot;red&quot;, lty = 2) + theme_bw() ggsave(here(&quot;result&quot;, &quot;poor.fit.png&quot;), plot=d1, dpi=300) The LM algorithm will help us find the correct parameters that minimize the error between the observed data and the model prediction. 4.2 Data fitting using nlsLM The nlsLM function in R (available from the minpack.lm package) implements the algorithm. It can optimize parameters in nonlinear least squares problems, like our damped oscillation model. To use nlsLM, we need to provide: The model function, The data, The initial guesses for the parameters. Here’s how we do it: f.do &lt;- function(x, A, Lambda, Omega, Pi, C){ ## to calculate y to use x and parameters y &lt;- A*exp(-Lambda*x)*cos(Omega*x+Pi)+C y } start &lt;- c(A=w0[1], Lambda = w0[2], Omega=w0[3], Pi=w0[4], C=w0[5]) m &lt;- nlsLM(y ~ f.do(x, A, Lambda, Omega, Pi, C), data = df_temp, start = start) summary(m) ## ## Formula: y ~ f.do(x, A, Lambda, Omega, Pi, C) ## ## Parameters: ## Estimate Std. Error t value Pr(&gt;|t|) ## A 1.9998538 0.0021139 946.1 &lt;2e-16 *** ## Lambda 0.1001776 0.0001714 584.5 &lt;2e-16 *** ## Omega 1.2565776 0.0001783 7048.5 &lt;2e-16 *** ## Pi 0.4980359 0.0010815 460.5 &lt;2e-16 *** ## C 0.9997494 0.0005270 1896.9 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.002829 on 25 degrees of freedom ## ## Number of iterations to convergence: 3 ## Achieved convergence tolerance: 1.49e-08 After running this, the algorithm returns optimized parameters that are very close to the real values we started with. The iteration cycle is just 3 iterations, showing that the algorithm is efficient and quickly finds the best parameters. The optimized parameters are almost identical to the real values used to generate the data: n1 &lt;- unlist(coef(m)) n1 ## A Lambda Omega Pi C ## 1.9998538 0.1001776 1.2565776 0.4980359 0.9997494 Now, we can visualize the result. The optimized parameters (black line) fit the data much better than the initial guess (red line), as shown below: d2 &lt;- ggplot(df_temp, aes(x, y)) + geom_point() + stat_function(fun = function(x) w0[1] * exp(-w0[2] * x) * cos(w0[3] * x + w0[4]) + w0[5], col = &quot;red&quot;, lty = 2) + # Initial guess stat_function(fun = function(x) n1[1] * exp(-n1[2] * x) * cos(n1[3] * x + n1[4]) + n1[5], col = &quot;black&quot;, lty = 2) + # Optimized result theme_bw() ggsave(here(&quot;result&quot;,&quot;good.fit.png&quot;), plot=d2, dpi=300) 4.3 How the Algorithm Works The algorithm is a hybrid method that combines the Gauss-Newton method and gradient descent method. It is used to find the best-fitting parameters for models that involve nonlinear relationships, like our damped oscillation model. Here’s a basic breakdown of how the algorithm works: Initial Parameters: The algorithm starts with an initial guess for the parameters. Model Evaluation: The model (i.e., our damped oscillation formula) is evaluated with these initial parameters, and the error between the predicted values and the actual data is calculated (this is usually the chi-squared error). Jacobian Matrix: The algorithm computes the Jacobian matrix, which describes how sensitive the model is to changes in each parameter. Adjusting Parameters: Based on the Jacobian and the error, the algorithm updates the parameters to minimize the error. It adjusts the parameters using a mixture of Gauss-Newton (which is fast but can be unstable) and gradient descent (which is slower but more stable). Convergence: This process is repeated, adjusting the parameters until the error becomes small enough, indicating that the model has converged to the optimal parameter values. 4.4 Custom Implementation of the Algorithm To understand how the nlsLMfunction works, we can implement a simplified version of the algorithm ourselves in R. Below is a my custom implementation, where we control the damping factor \\(\\mu\\) and adjust it during the iterations. 4.5 Convergence Criteria of the Algorithm In the algorithm, the goal is to iteratively adjust the model parameters (w) so that the model’s predictions closely match the observed data. The algorithm stops iterating when it meets certain convergence criteria — conditions that indicate the algorithm has found a good solution. These conditions are based on different aspects of the optimization process. Here are the convergence criteria used in my code: Actual and Predicted Relative Reduction in Chi-Squared The actual relative reduction (ARD) is the change in chi-squared based on the real improvement from the current model to the next model after parameter updates. It’s computed as: \\[ \\text{actual.relative.reduction} = \\frac{\\text{previous.chisq} - \\text{chisq.new}}{\\text{previous.chisq}} \\] Here, previous.chisq is the chi-squared value from the previous iteration (or from the initial iteration), and chisq.new is the chi-squared value after the parameter update. The ARD tells us how much the chi-squared has improved due to the changes in the model’s parameters. The predicted relative reduction (PRD) is an estimate of the expected reduction in chi-squared, based on the Jacobian matrix and the model’s current residuals. It is computed as: \\[ \\text{predicted.relative.reduction} = \\frac{\\chi^2(\\theta_{old})-\\chi^2(\\theta_{new})}{|d^T_{k}\\cdot(damp \\cdot d_k + J^Tr)|} \\] This prediction is made based on the current parameter update and the model’s Jacobian matrix. It estimates how much the chi-squared is expected to change based on the current step size and damping. In some cases, the predicted reduction can indicate whether a larger or smaller parameter update is required. To ensure that the algorithm has converged, both ARD and PRD must be below a specified threshold (commonly denoted as ftol). This signals that further iterations will not yield significant improvements in the fit. The algorithm will terminate when the following conditions are met: Convergence Condition: The algorithm will stop if both the ARD and the PRD are below ftol: \\[ \\text{actual.relative.reduction} &lt; \\text{ftol} \\quad \\text{and} \\quad p &lt; \\text{ftol} \\] ftol is a small positive number (e.g., \\(1 \\times 10^{-6}\\)), which sets the tolerance level for the relative decrease in chi-squared. If both the actual and predicted reductions are smaller than this threshold, it indicates that the optimization has plateaued, meaning no significant improvement is expected in subsequent iterations. Why Two Criteria? ARD measures how much the chi-squared has decreased in reality after the parameter update. If this value is very small, it means that the model is no longer significantly improving. PRD estimates how much the chi-squared is expected to decrease in the next iteration, based on the current model’s residuals and the Jacobian matrix. If the predicted reduction is small, it means that the algorithm expects little improvement, even if it continues. By comparing both reductions (actual and predicted), the algorithm avoids prematurely stopping when only one of them is small but still allows for stopping when both suggest minimal improvement. Relative Error in Parameter Updates (RE): This is the largest relative change in the model parameters (dk), normalized by the size of the current parameter values (w). If the changes in the model parameters are very small (below a threshold, ptol), it means that the parameters have stabilized, and further iterations are unlikely to improve the model significantly. Cosine Angles Between Residuals and Jacobian Columns (CA): The cosine angles measure how closely the residuals (differences between the predicted and observed values) align with the columns of the Jacobian matrix (which represents the sensitivity of the model to each parameter). If the cosine angles are very small, it indicates that the parameter updates are not pointing in a useful direction for improving the fit (i.e., the algorithm is “stuck”). The algorithm stops when the cosine angles are below a threshold (gtol). Parameter updating and adjusting \\(\\mu\\): In order to update parameters, the goodness-of-fit is evaluated by computation of the predicted relative reduction: where, \\(\\lambda_i\\) is \\(\\mu_i\\), If \\(\\lambda_0\\) = \\(\\lambda_o\\) is the default set (1e-2) and If PRD &gt; 0 and \\(\\alpha\\) = \\(\\alpha\\)+\\(h_{lm}\\) and \\(\\lambda_{i+1}\\) = max[\\(\\lambda_i\\)/\\(L_{1}\\), 1e-7], Otherwise \\(\\lambda_i\\) = min[\\(\\lambda_i\\) \\(L_{2}\\), 1e7] \\(L_{1} \\approx9\\) and \\(L_{2} \\approx 11\\) Weight \\(W\\): I do not apply weights, even for nlsLM, because not applying weights is equivalent to multiplying by 1, which does not affect the result. Convergence The custom code will stop if any of the following conditions are met: The relative reduction in chi-squared is very small (i.e., the model is no longer improving significantly). The relative error in the parameter updates is very small (i.e., the parameters are not changing much). The cosine angles between residuals and Jacobian columns are small (i.e., the updates are not pointing in a useful direction). 4.6 Code Example LM.custom &lt;- function(w0, data) { w &lt;- w.previous &lt;- w0 k &lt;- 0 feval &lt;- 0 mu &lt;- 1e-2 ftol &lt;- 1e-6 ptol &lt;- 1e-6 gtol &lt;- 0 maxiter &lt;- 200 maxfev &lt;- 600 x &lt;- data$x y &lt;- data$y f &lt;- function(w0){ model &lt;- (w0[1]*exp(-w0[2]*x)*cos(w0[3]*x + w0[4]) + w0[5]) return(model) } chi.squre.fn &lt;- function(w) { chi.sq &lt;- sum((y - f(w))^2) return(chi.sq) } norm.fn &lt;- function(x) sqrt(sum(x^2)) initial.chisq &lt;- previous.chisq &lt;- chi.squre.fn(w) calculate.cosine.angles &lt;- function(y, w) { r &lt;- y - f(w) J &lt;- numDeriv::jacobian(f, w) norm.r &lt;- sqrt(sum(r^2)) cosine.angles &lt;- numeric(ncol(J)) for (i in 1:ncol(J)) { J.col &lt;- J[, i] norm.J.col &lt;- sqrt(sum(J.col^2)) dot.product &lt;- sum(J.col * r) cosine.angles[i] &lt;- dot.product / (norm.J.col * norm.r) } return(cosine.angles) } while (k &lt; maxiter &amp;&amp; feval &lt; maxfev) { J &lt;- numDeriv::jacobian(f, w, method = &quot;Richardson&quot;) r1 &lt;- y - f(w) Jr &lt;- t(J) %*% r1 feval &lt;- feval + 1 if (feval == maxfev) { cat(&quot;Number of function evaluations reached: &quot;, feval, &quot;\\n&quot;) break } JTJ &lt;- t(J) %*% J + mu * diag(ncol(J)) damp &lt;- mu * diag(ncol(JTJ)) JTJ.inv &lt;- tryCatch({ qr.solve(JTJ) }, error = function(e) { ginv(JTJ) }) dk &lt;- JTJ.inv %*% Jr w1 &lt;- w + dk chisq.new &lt;- chi.squre.fn(w1) actual.relative.reduction &lt;- abs(initial.chisq - chisq.new) / initial.chisq predicted.relative.reduction &lt;- p &lt;- (previous.chisq - chisq.new) / abs(t(dk) %*% (damp %*% dk + Jr)) relative.error &lt;- max(abs(w1 - w.previous)) / max(abs(w.previous)) cosine.angles &lt;- calculate.cosine.angles(y, w) convergence_result &lt;- c() convergence &lt;- function() { if (actual.relative.reduction &lt; ftol &amp; p &lt; ftol) { print(paste(&quot;ARD:&quot;, actual.relative.reduction, &quot;PRD:&quot;, p)) } else if (relative.error &lt; ptol) { print(paste(&quot;RE:&quot;, relative.error)) } else { print(paste(&quot;CA:&quot;, cosine.angles)) } } if (actual.relative.reduction &lt; ftol &amp; p &lt; ftol || relative.error &lt; ptol || max(abs(cosine.angles)) &lt; gtol) { convergence_result &lt;- convergence() break } if (p &gt; 0) { mu &lt;- max(mu / 9, 1e-7) w.previous &lt;- w w &lt;- w1 } else { mu &lt;- min(mu * 11, 1e7) } k &lt;- k + 1 previous.chisq &lt;- chisq.new } sigma.sq &lt;- sum(r1^2) / (length(y) - length(w)) cov.matrix &lt;- ginv(JTJ) * sigma.sq se &lt;- sqrt(diag(cov.matrix)) result &lt;- list(parameters = w, standard.errors = se, iteration = k, convergence.result = convergence_result) return(result) } To run the code, do as shown below. LM.custom(w0, df_temp) ## [1] &quot;RE: 1.12795454748581e-09&quot; ## $parameters ## [,1] ## [1,] 1.9998538 ## [2,] 0.1001776 ## [3,] 1.2565776 ## [4,] 0.4980359 ## [5,] 0.9997494 ## ## $standard.errors ## [1] 0.0021138956 0.0001713882 0.0001782752 0.0010815145 0.0005270499 ## ## $iteration ## [1] 4 ## ## $convergence.result ## [1] &quot;RE: 1.12795454748581e-09&quot; The custom version of the algorithm also returns very similar results to the nlsLM estimated result (refer to the below) showing that the algorithm is efficient and quickly finds the best parameters. n1 ## A Lambda Omega Pi C ## 1.9998538 0.1001776 1.2565776 0.4980359 0.9997494 "],["applying-the-algorithm-to-a-sphere-function-model.html", "Chapter 5 Applying the Algorithm to a Sphere function model 5.1 Code Explanation 5.2 Example data 5.3 Optimization with the Algorithm 5.4 Applying the Algorithm 5.5 Code Example", " Chapter 5 Applying the Algorithm to a Sphere function model Let’s have an extra example. Here we aim to visualize a well-known mathematical function called the Sphere function, which is used frequently in optimization problems. It is defined as: \\[ f(x) = x_1^2 + x_2^2 + \\dots + x_n^2 \\] Where \\(x\\) represents the input vector, and each component \\(x_i\\) is squared and summed. The Sphere function simply calculates how far a given point is from the origin (0, 0, …, 0) in multi-dimensional space. 5.1 Code Explanation We define the Sphere function (s.function) which calculates the sum of squares of the components of a vector x. For example, for two dimensions, if \\(x = (x_1, x_2)\\), the function will calculate \\(x_1^2 + x_2^2\\). We then create a grid of input values for \\(x\\) and \\(y\\) in a 2D space. This grid is used to compute the corresponding cost values (or Sphere function values) across the space. After calculating the cost values for all the grid points, we find the point where the Sphere function reaches its minimum (which is at the origin, where all the components of x are zero). Finally, we create a 3D surface plot using Plotly, which allows us to visualize the cost landscape of the Sphere function. The red dot on the plot represents the minimum cost, which corresponds to the point (0,0) in this example. 5.2 Example data This is a 3D plot of the Sphere function where the height (z-axis) shows the function’s value at each point. The red point indicates the minimum cost point, which is the point where the Sphere function reaches its lowest value (at the origin). s.function &lt;- function(x) { return(sum(x^2)) } x.range &lt;- seq(-10, 10, by = 0.1) y.range &lt;- seq(-10, 10, by = 0.1) cost.matrix &lt;- matrix(NA, nrow = length(x.range), ncol = length(y.range)) for (i in 1:length(x.range)) { for (j in 1:length(y.range)) { cost.matrix[i, j] &lt;- s.function(c(x.range[i], y.range[j])) } } rownames(cost.matrix) &lt;- seq(-10, 10, by = .1) colnames(cost.matrix) &lt;- seq(-10, 10, by = .1) min.cost &lt;- min(cost.matrix) min.cost.pos &lt;- which(cost.matrix == min.cost, arr.ind = TRUE) theta0.min &lt;- rownames(cost.matrix)[min.cost.pos[1]] theta1.min &lt;- colnames(cost.matrix)[min.cost.pos[2]] plot_ly(z = cost.matrix, type = &quot;surface&quot;, x = as.numeric(colnames(cost.matrix)), y = as.numeric(rownames(cost.matrix))) %&gt;% add_trace( type = &quot;scatter3d&quot;, mode = &quot;markers&quot;, x = as.numeric(theta1.min), y = as.numeric(theta0.min), z = min.cost, marker = list(size = 5, color = &#39;red&#39;) ) %&gt;% layout( scene = list( xaxis = list(title = &quot;X&quot;), yaxis = list(title = &quot;X&quot;), zaxis = list(title = &quot;f(X)&quot;) ) ) 5.3 Optimization with the Algorithm Now, we implement the LM algorithm to minimize the Sphere function or find the values of \\(x\\) that make \\(f(x)\\) as small as possible. Here is the key concepts: Sphere Function: This is the function we’re trying to minimize. It’s the same function we used in the previous step. Gradient: The derivative of the Sphere function with respect to each input component \\(x_i\\), which is used to find the direction of steepest descent. The gradient of the Sphere function is simply \\(2x\\), meaning that at each point, the direction to move to minimize the function is directly opposite the value of the current point. Jacobian: The Jacobian matrix is a matrix of all the first-order partial derivatives of the function with respect to its parameters. In this case, the function is a sum of squares, so the Jacobian is simply a vector of partial derivatives. The algorithm iteratively updates the parameters (in this case, the values of \\(x\\)) using a combination of the gradient and a damping factor (denoted as \\(\\mu\\)). The algorithm adjusts the damping factor at each step to ensure convergence to the minimum. The algorithm checks for convergence at each step by evaluating the change in the cost function and the convergence criteria as shown before, only I slightly modified the criteria for the given example yet the main algorithm remains untouched. 5.4 Applying the Algorithm We start with an initial guess for the parameters w0 = (10, 10), which are far from the minimum of the Sphere function at (0, 0). The LM.custom function is applied to minimize the Sphere function starting from these initial values. After running the algorithm, we obtain the optimized parameter values, which should be close to (0, 0), the true minimum of the Sphere function. 5.5 Code Example LM.custom &lt;- function(w0) { w &lt;- w.previous &lt;- w0 k &lt;- 0 feval &lt;- 0 mu &lt;- 1e-2 ftol &lt;- 1e-6 ptol &lt;- 1e-6 maxiter &lt;- 200 maxfev &lt;- 600 f &lt;- function(x) { return(sum(x^2)) } residual &lt;- function(w) { return(f(w)) } chi.squre.fn &lt;- function(w) { chi.sq &lt;- residual(w) return(chi.sq) } norm.fn &lt;- function(x) sqrt(sum(x^2)) initial.chisq &lt;- previous.chisq &lt;- chi.squre.fn(w) while (k &lt; maxiter &amp;&amp; feval &lt; maxfev) { J &lt;- numDeriv::jacobian(f, w, method = &quot;Richardson&quot;) r1 &lt;- residual(w) Jr &lt;- t(J) %*% r1 feval &lt;- feval + 1 if (feval == maxfev) { cat(&quot;Number of function evaluations reached: &quot;, feval, &quot;\\n&quot;) break } JTJ &lt;- t(J) %*% J + mu * diag(ncol(J)) QR &lt;- qr(JTJ) JTJ.inv &lt;- tryCatch({ qr.solve(JTJ) }, error = function(e) { ginv(JTJ) }) damp &lt;- mu * diag(ncol(JTJ)) dk &lt;- JTJ.inv %*% Jr w1 &lt;- w - dk chisq.new &lt;- chi.squre.fn(w1) actual.relative.reduction &lt;- abs(initial.chisq - chisq.new) / initial.chisq predicted.relative.reduction &lt;- p &lt;- (previous.chisq - chisq.new) / abs(t(dk) %*% (damp %*% dk + Jr)) relative.error &lt;- max(abs(w1 - w.previous)) / max(abs(w.previous)) convergence_result &lt;- c() convergence &lt;- function() { if (actual.relative.reduction &lt; ftol &amp; p &lt; ftol) { print(paste(&quot;ARD:&quot;, actual.relative.reduction, &quot;PRD:&quot;, p)) } else { print(paste(&quot;RE:&quot;, relative.error)) } } if (actual.relative.reduction &lt; ftol &amp; p &lt; ftol || relative.error &lt; ptol) { convergence_result &lt;- convergence() break } if (p &gt; 0) { mu &lt;- max(mu / 9, 1e-7) w.previous &lt;- w w &lt;- w1 } else { mu &lt;- min(mu * 11, 1e7) } previous.chisq &lt;- chisq.new k &lt;- k + 1 } result &lt;- list(parameters = w, iteration = k, convergence.result = convergence()) return(result) } We can run the code as follows. # Define the initial guess for the parameters w0 &lt;- c(10, 10) # Apply the LM algorithm to minimize the Sphere function (LM.custom(w0)) ## [1] &quot;RE: 0.00548784283660737&quot; ## $parameters ## [,1] ## [1,] 8.276741e-06 ## [2,] 8.276741e-06 ## ## $iteration ## [1] 200 ## ## $convergence.result ## [1] &quot;RE: 0.00548784283660737&quot; The estimated parameters are close to (0, 0), while the iteration stopped because the maximum iteration limit of 200 cycles was reached. This means that the estimated parameters are only shown as a result, not because the algorithm converged. "],["applying-the-lm-algorithm-to-lba-data.html", "Chapter 6 Applying the LM Algorithm to LBA data 6.1 5 PL curve 6.2 Data set 6.3 Initial parameter estimation 6.4 Example of Code 6.5 Visualization of the fitting process 6.6 Result", " Chapter 6 Applying the LM Algorithm to LBA data In this chapter, we’ll apply the LM algorithm to fit a 5 parameter logistic (5 PL) curve to LBA data, specifically focusing on calibration curve fitting. 6.1 5 PL curve The 5PL curve is commonly used to describe non-linear relationships, such the one found in bioassay where the assay response changes as the concentration of a substance increases. The 5PL equation is: \\[ Response = D+ \\frac{A-D}{\\left(1+\\left(\\frac{x}{c}\\right)^B \\right)^G} \\] Symbol Parameter Name Notice D w1 Infinite x asymptote The maximum response (upper asymptote) B w2 Slope/Hill Controls how steep the curve is (representing the rate of change of response) C w3 Inflection point The concentration where the steepest change occurs. A w4 Small x asymptote The minimum response (lower asymptote) G w5 Asymmetric factor Controls the symmetry of the curve (how “skewed” it is) If the assay response increases with concentration, the parameters are used as described. If the assay response decreases with increasing concentration, parameters A and D should be switched. 6.2 Data set We begin with pseudo datasets containing the assay responses (OD values) of calibration standard samples. Each run includes 12 levels of calibration standard samples, with a total of over 100 runs. Each standard sample was measured in duplicate. The first 2 and the last 3 levels were not used for quantification but not excluded from the fitting process, as they help curve fitting. These are called anchor points. Below is an example of how to prepare and structure the data to make a curve fitting for each run: df_temp.0 &lt;- read.csv(here(&quot;data&quot;,&quot;cal.std.od.csv&quot;), header = F) od &lt;- c() for(i in 1:nrow(df_temp.0)) { od &lt;- rbind(od, t(df_temp.0[i,])) } calib.0 &lt;- c() for (i in 1:12){ if(i &lt; 10){ calib.1 &lt;- paste0(&quot;STD0&quot;, i) calib.0 &lt;- c(calib.0, calib.1) } else{ calib.1 &lt;- paste0(&quot;STD&quot;, i) calib.0 &lt;- c(calib.0, calib.1) } } conc.0 &lt;- c(5, 10, 15, 30, 60, 80, 100, 150, 200, 400, 800, 1600) rep.0 &lt;- rep(c(1, 2), each=12) run.no &lt;- nrow(df_temp.0)/2 calib &lt;- rep(calib.0, 2*run.no) conc &lt;- rep(conc.0, 2*run.no) rep &lt;- rep(rep.0, run.no) run &lt;- rep(seq(1, run.no), each=24) df_temp &lt;- data.frame(calib, conc, rep, od, run) colnames(df_temp) &lt;- c(&quot;calb&quot;, &quot;conc&quot;, &quot;rep&quot;, &quot;od&quot;, &quot;run&quot;) The data structure is as shown below (it is called long form data). head(df_temp, 30) ## calb conc rep od run ## V1 STD01 5 1 0.032825 1 ## V2 STD02 10 1 0.040097 1 ## V3 STD03 15 1 0.071710 1 ## V4 STD04 30 1 0.193617 1 ## V5 STD05 60 1 0.319059 1 ## V6 STD06 80 1 0.495708 1 ## V7 STD07 100 1 0.595597 1 ## V8 STD08 150 1 0.871226 1 ## V9 STD09 200 1 1.411576 1 ## V10 STD10 400 1 1.575802 1 ## V11 STD11 800 1 2.375823 1 ## V12 STD12 1600 1 2.786388 1 ## V1.1 STD01 5 2 0.027876 1 ## V2.1 STD02 10 2 0.035754 1 ## V3.1 STD03 15 2 0.067872 1 ## V4.1 STD04 30 2 0.181901 1 ## V5.1 STD05 60 2 0.321483 1 ## V6.1 STD06 80 2 0.506717 1 ## V7.1 STD07 100 2 0.596607 1 ## V8.1 STD08 150 2 0.854561 1 ## V9.1 STD09 200 2 1.517323 1 ## V10.1 STD10 400 2 1.842644 1 ## V11.1 STD11 800 2 2.534292 1 ## V12.1 STD12 1600 2 3.490257 1 ## V1.2 STD01 5 1 0.040804 2 ## V2.2 STD02 10 1 0.039087 2 ## V3.2 STD03 15 1 0.063529 2 ## V4.2 STD04 30 1 0.140289 2 ## V5.2 STD05 60 1 0.321988 2 ## V6.2 STD06 80 1 0.430866 2 6.3 Initial parameter estimation Before fitting the 5PL curve, we need to estimate the initial values for the parameters.The initial values will also be directly estimated from the calibration standard sample data.Here’s how to compute them: w1: average of the maximum assay response (Max) w4: average of the minimum assay response (Min) Next, we calculate the logit transformation to linearize the data: \\[ Logit(Y) = ln \\left( \\frac{Y-Min}{Max-Y}\\right)\\] We then fit a simple linear model using the transformed data and estimate the parameters: \\[ logit(y) = intercept \\ + slope*ln(X) \\] and, w2: abs(slope) w3: EXP(-intercept / slope) w5: 1 We can automatically compute parameters values for each run by running the following code: sumStart &lt;- function(rawdata){ df_temp.0 &lt;- data.frame(run=unique(rawdata$run), Max = NA, Slope = NA, C = NA, Min = NA, M= NA) run.list &lt;- unique(rawdata$run) for (i in run.list) { df_temp.i &lt;- rawdata[rawdata$run == i, ] max &lt;- mean(c(df_temp.i$od[df_temp.i$rep == 1 &amp; df_temp.i$conc==max(df_temp.i$conc)], df_temp.i$od[df_temp.i$rep == 2 &amp; df_temp.i$conc==max(df_temp.i$conc)])) min &lt;- mean(c(df_temp.i$od[df_temp.i$rep == 1 &amp; df_temp.i$conc==min(df_temp.i$conc)], df_temp.i$od[df_temp.i$rep == 2 &amp; df_temp.i$conc==min(df_temp.i$conc)])) df_temp.i$logit.y &lt;- log((df_temp.i$od-min)/(max-df_temp.i$od)) df_temp.i$logit.y[is.nan(df_temp.i$logit.y)|is.infinite(df_temp.i$logit.y)] &lt;- NA model &lt;- lm(logit.y~log(conc), df_temp.i) slope &lt;- coef(model)[[2]] B &lt;- abs(slope) ed50 &lt;- exp(-coef(model)[[1]]/slope) df_temp.0$Max[df_temp.0$run == i] &lt;- max df_temp.0$Min[df_temp.0$run == i] &lt;- min df_temp.0$C[df_temp.0$run == i] &lt;- ed50 df_temp.0$Slope[df_temp.0$run == i] &lt;- B df_temp.0$M[df_temp.0$run == i] &lt;- 1 } df_temp.0 } df_start &lt;- sumStart(df_temp) As the result, the initial parameters for each run will be constructed as shown below. head(df_start, 10) ## run Max Slope C Min M ## 1 1 3.138323 1.559031 293.7739 0.0303505 1 ## 2 2 3.250432 1.687841 288.8220 0.0391880 1 ## 3 3 3.309871 1.605703 250.8145 -0.0742350 1 ## 4 4 3.553180 1.563305 270.0125 0.0263610 1 ## 5 5 3.779066 1.499061 298.6959 0.0225735 1 ## 6 6 3.568785 1.781262 243.4372 0.0254520 1 ## 7 7 3.618577 1.675017 224.6187 0.0312090 1 ## 8 8 3.452584 1.596580 242.7640 0.0289365 1 ## 9 9 3.094943 1.517485 366.1966 0.0175235 1 ## 10 10 3.070198 1.507721 315.1467 0.0210080 1 6.4 Example of Code Here is my custom code to apply the LM algorithm to 5PL curve fitting. This code not only fits the data (optimization of parameters) but also saves useful information obtained from each iteration, specifically to visualize the iteration process so we can see how a calibration curve fits the data (refer to the codes marked with #) . LM.custom &lt;- function(w0, data, run) { w &lt;- w.previous &lt;- w0 k &lt;- 0 feval &lt;- 0 mu &lt;- 1e-2 ftol &lt;- 1e-6 ptol &lt;- 1e-6 gtol &lt;- 0 maxiter &lt;- 200 maxfev &lt;- 600 y &lt;- data$od[data$run == run] x_data &lt;- unique(data$conc) n &lt;- length(x_data) df_temp &lt;- data.frame(x = unique(data$conc), od = y) f &lt;- function(w0) { model &lt;- w0[1] + (w0[4] - w0[1]) / (1 + (df_temp$x / w0[3])^w0[2])^w0[5] return(model) } chi.squre.fn &lt;- function(w) { chi.sq &lt;- sum((y - f(w))^2) return(chi.sq) } norm.fn &lt;- function(x) sqrt(sum(x^2)) initial.chisq &lt;- previous.chisq &lt;- chi.squre.fn(w) calculate.cosine.angles &lt;- function(y, w) { r &lt;- y - f(w) J &lt;- numDeriv::jacobian(f, w) norm.r &lt;- sqrt(sum(r^2)) cosine.angles &lt;- numeric(ncol(J)) for (i in 1:ncol(J)) { J.col &lt;- J[, i] norm.J.col &lt;- sqrt(sum(J.col^2)) dot.product &lt;- sum(J.col * r) cosine.angles[i] &lt;- dot.product / (norm.J.col * norm.r) } return(cosine.angles) } df_animation &lt;- data.frame(iteration = integer(0), x = numeric(0), fitted.y = numeric(0), od = numeric(0)) # while (k &lt; maxiter &amp;&amp; feval &lt; maxfev) { J &lt;- numDeriv::jacobian(f, w) r1 &lt;- y - f(w) Jr &lt;- t(J) %*% r1 feval &lt;- feval + 1 if (feval == maxfev) { cat(&quot;Number of function evaluations reached: &quot;, feval, &quot;\\n&quot;) break } JTJ &lt;- t(J) %*% J + mu * diag(ncol(J)) QR &lt;- qr(JTJ) damp &lt;- mu * diag(ncol(JTJ)) JTJ.inv &lt;- tryCatch({ qr.solve(JTJ) }, error = function(e) { ginv(JTJ) }) dk &lt;- JTJ.inv %*% Jr w1 &lt;- w + dk chisq.new &lt;- chi.squre.fn(w1) actual.relative.reduction &lt;- abs( previous.chisq - chisq.new) / previous.chisq predicted.relative.reduction &lt;- p &lt;- (previous.chisq - chisq.new) / abs(t(dk) %*% (damp %*% dk + Jr)) relative.error &lt;- max(abs(w1 - w.previous)) / max(abs(w.previous)) cosine.angles &lt;- calculate.cosine.angles(y, w) convergence.result &lt;- c() convergence &lt;- function() { if (actual.relative.reduction &lt; ftol &amp; p &lt; ftol) { convergence.result &lt;- paste(&quot;ARD:&quot;, actual.relative.reduction, &quot;\\n&quot;, &quot;PRD:&quot;, p ) } else if (relative.error &lt; ptol) { convergence.result &lt;- paste(&quot;RE:&quot;, relative.error) } else { convergence.result &lt;- paste(&quot;CA:&quot;, cosine.angles) } } convergence.result &lt;- c() if (actual.relative.reduction &lt; ftol &amp; p &lt; ftol || relative.error &lt; ptol || max(abs(cosine.angles)) &lt; gtol ) { convergence.result &lt;- convergence() print(convergence.result) print(w) break } if (p &gt; 0) { mu &lt;- max(mu / 9, 1e-10) w_previous &lt;- w w &lt;- w1 } else { mu &lt;- min(mu * 11, 1e10) } previous.chisq &lt;- chisq.new k &lt;- k + 1 fitted.y &lt;- f(w) # df_animation &lt;- rbind(df_animation, data.frame(iteration = k, x = df_temp$x, fitted.y = fitted.y, od = df_temp$od)) # if (k == maxiter) { cat(&quot;Number of iterations till stop: &quot;, k, &quot;\\n&quot;) break } } return(df_animation) } 6.5 Visualization of the fitting process To visualize a specific calibration curve, for example, that of run no.2, we need to specify the run first and its parameters, before running the code above as follows: n &lt;- 2 w &lt;- c(df_start$Max[df_start$run==n], df_start$Slope[df_start$run==n], df_start$C[df_start$run==n], df_start$Min[df_start$run==n], df_start$M[df_start$run==n]) And now we can run the main code as follows: test.result &lt;- LM.custom(w, df_temp, n) ## [1] &quot;ARD: 1.38822739141541e-15 \\n PRD: -24911.2076465574&quot; ## [,1] ## [1,] 6.58918714 ## [2,] 1.32383575 ## [3,] 193.61811188 ## [4,] 0.01556511 ## [5,] 0.23739597 And by running the following code, we can create an animation of the curve fitting process. animation &lt;- ggplot(test.result, aes(log(x), fitted.y, color = as.factor(iteration))) + geom_line() + geom_point(aes(log(x), od), color = &quot;black&quot;, alpha = 0.5) + labs(title = &#39;Iteration: {closest_state}&#39;, x = &#39;Concentration&#39;, y = &#39;OD&#39;) + scale_x_continuous(breaks = c(log(5), log(10), log(15), log(30), log(60), log(80), log(100), log(150), log(200), log(400), log(800), log(1600)), labels = c(&quot;5&quot;, &quot;10&quot;, &quot;15&quot;, &quot;30&quot;, &quot;60&quot;, &quot;80&quot;, &quot;100&quot;, &quot;150&quot;, &quot;200&quot;, &quot;400&quot;, &quot;800&quot;, &quot;1600&quot;)) + theme_bw() + theme(axis.text.x = element_text(angle = 45, hjust = 1)) + transition_states(iteration, transition_length = 1, state_length = 1) + ease_aes(&#39;linear&#39;) + scale_color_viridis_d()+ ylab(&quot;OD&quot;)+ xlab(&quot;Concentration (ng/mL) in log sacle&quot;) anim_save(here(&quot;result&quot;, paste(&quot;animation02.gif&quot;)), animation = animation) 6.6 Result Now we have the animated figure. This is another figure obtained from another run. ## [1] &quot;ARD: 1.04836759881908e-14 \\n PRD: -50.5868608662427&quot; ## [,1] ## [1,] 5.49125202 ## [2,] 1.04675251 ## [3,] 273.36582074 ## [4,] -0.01495244 ## [5,] 0.25593591 By introducing some modifications into the code, we can create plots showing the changes in each convergence criterion during the iterations. The following code has been modified to achieve this (refer to some codes marked with #). LM.custom.2 &lt;- function(w0, data, run) { w &lt;- w.previous &lt;- w0 k &lt;- 0 feval &lt;- 0 mu &lt;- 1e-2 ftol &lt;- 1e-6 ptol &lt;- 1e-6 gtol &lt;- 0 maxiter &lt;- 200 maxfev &lt;- 600 y &lt;- data$od[data$run == run] x_data &lt;- unique(data$conc) n &lt;- length(x_data) df_temp &lt;- data.frame(x = unique(data$conc), od = y) f &lt;- function(w0) { model &lt;- w0[1] + (w0[4] - w0[1]) / (1 + (df_temp$x / w0[3])^w0[2])^w0[5] return(model) } chi.squre.fn &lt;- function(w) { chi.sq &lt;- sum((y - f(w))^2) return(chi.sq) } norm.fn &lt;- function(x) sqrt(sum(x^2)) initial.chisq &lt;- previous.chisq &lt;- chi.squre.fn(w) calculate.cosine.angles &lt;- function(y, w) { r &lt;- y - f(w) J &lt;- numDeriv::jacobian(f, w) norm.r &lt;- sqrt(sum(r^2)) cosine.angles &lt;- numeric(ncol(J)) for (i in 1:ncol(J)) { J.col &lt;- J[, i] norm.J.col &lt;- sqrt(sum(J.col^2)) dot.product &lt;- sum(J.col * r) cosine.angles[i] &lt;- dot.product / (norm.J.col * norm.r) } return(cosine.angles) } df_result &lt;- data.frame(Iteration = integer(0), RE = numeric(0), ARD = numeric(0), PRD= numeric(), CA = numeric(0), mu = numeric(0)) # while (k &lt; maxiter &amp;&amp; feval &lt; maxfev) { J &lt;- numDeriv::jacobian(f, w) r1 &lt;- y - f(w) Jr &lt;- t(J) %*% r1 feval &lt;- feval + 1 if (feval == maxfev) { cat(&quot;Number of function evaluations reached: &quot;, feval, &quot;\\n&quot;) break } JTJ &lt;- t(J) %*% J + mu * diag(ncol(J)) QR &lt;- qr(JTJ) damp &lt;- mu * diag(ncol(JTJ)) JTJ.inv &lt;- tryCatch({ qr.solve(JTJ) }, error = function(e) { ginv(JTJ) }) dk &lt;- JTJ.inv %*% Jr w1 &lt;- w + dk chisq.new &lt;- chi.squre.fn(w1) actual.relative.reduction &lt;- abs( previous.chisq - chisq.new) / previous.chisq predicted.relative.reduction &lt;- p &lt;- (previous.chisq - chisq.new) / abs(t(dk) %*% (damp %*% dk + Jr)) relative.error &lt;- max(abs(w1 - w.previous)) / max(abs(w.previous)) cosine.angles &lt;- calculate.cosine.angles(y, w) df_result &lt;- rbind(df_result, data.frame(Iteration = k, RE = relative.error, ARD = actual.relative.reduction, PRD = p, CA = max(abs(cosine.angles)), mu = mu)) # convergence &lt;- function() { if (actual.relative.reduction &lt; ftol &amp; p &lt; ftol) { convergence.result &lt;- paste(&quot;ARD:&quot;, actual.relative.reduction, &quot;\\n&quot;, &quot;PRD:&quot;, p ) } else if (relative.error &lt; ptol) { convergence.result &lt;- paste(&quot;RE:&quot;, relative.error) } else { convergence.result &lt;- paste(&quot;CA:&quot;, cosine.angles) } } convergence.result &lt;- c() if (actual.relative.reduction &lt; ftol &amp; p &lt; ftol || relative.error &lt; ptol || max(abs(cosine.angles)) &lt; gtol ) { convergence.result &lt;- convergence() print(convergence.result) print(w) break } if (p &gt; 0) { mu &lt;- max(mu / 9, 1e-10) w_previous &lt;- w w &lt;- w1 } else { mu &lt;- min(mu * 11, 1e10) } previous.chisq &lt;- chisq.new k &lt;- k + 1 if (k == maxiter) { cat(&quot;Number of iterations till stop: &quot;, k, &quot;\\n&quot;) break } } return(df_result) } The same way to run the code as follows: test.result.2 &lt;- LM.custom.2(w, df_temp, 74) ## [1] &quot;ARD: 1.04836759881908e-14 \\n PRD: -50.5868608662427&quot; ## [,1] ## [1,] 5.49125202 ## [2,] 1.04675251 ## [3,] 273.36582074 ## [4,] -0.01495244 ## [5,] 0.25593591 Finally, we can create figures to plot the convergence criteria values at each iteration: ARD &lt;- ggplot(test.result.2, aes(Iteration, ARD))+ geom_point(alpha=.5, size=.7)+ geom_line(col=&quot;red&quot;)+ theme_bw() PRD &lt;- ggplot(test.result.2, aes(Iteration, PRD))+ geom_point(alpha=.5, size=.7)+ geom_line(col = &quot;blue&quot;)+ theme_bw() RE &lt;- ggplot(test.result.2, aes(Iteration, RE))+ geom_point(alpha=.5, size=.7)+ geom_line(col=&quot;green&quot;)+ theme_bw() CA &lt;- ggplot(test.result.2, aes(Iteration, CA))+ geom_point(alpha=.5, size=.7)+ geom_line(col=&quot;purple&quot;)+ theme_bw() mu &lt;- ggplot(test.result.2, aes(Iteration, mu))+ geom_point(alpha=.5, size=.7)+ geom_line(col=&quot;skyblue&quot;)+ theme_bw() p &lt;- grid.arrange(ARD, PRD, RE, CA, mu, nrow=2) ggsave(here(&quot;result&quot;, &quot;convergence.criteria.png&quot;), plot=p, dpi=300) "],["evaluation-of-the-custom-code.html", "Chapter 7 Evaluation of the custom code", " Chapter 7 Evaluation of the custom code From a practical perspective, it makes sense to compare both functions (nlsLM and the custom code) to ensure that the custom code works properly for the calibration curve fitting because the estimated parameters might not be identical due to the inherent characteristic of this optimization algorithm since it aims to find local minima rather than a global minimum. Therefore, the direct comparison of the parameters is not practical and that also may lead to inappropriate interpretations. For those reasons, we’ll apply more intuitive method based on the general principle and practice of the analytical method validation. First, we’ll optimize parameters of each run using my code and nlsLM, independently. Next, using the parameters we’ll compute back-calculated concentrations of each calibration standard to evaluate accuracy of those. The accuracy can be expressed and measured by %Bias such as: \\[ \\text{%Bias} =\\frac{\\text{(Back-calculated Concentration - Nominal Concentration)}}{\\text{Nomincal Concentration}} \\cdot 100 \\] Second, we’ll plot the %Bias result so can compare those visually. In most of cases, the acceptable accuracy range for LLOQ and ULOQ calibration standard is \\(\\pm\\) 25 %Bis, and \\(\\pm\\) 20 %Bias for the others. These do not usually apply to anchor point(s), but this time we’ll do to compare the %Bias of all calibration standards, to check how they differ, and to determine if the differences are within the acceptance criteria for accuracy, despite any discrepancies. In my experience, it is a good practice to evaluate results carefully, as not all commercial analytical software provides the same level of transparency. This is particularly important when using different software, since users cannot always be sure of the algorithms being used or whether there are differences in certain aspects, even when the same algorithm is applied. Here’s an example code. By running this code, we can obtain optimized parameter from each run, which are estimated by my code and save them as result for %Bias calculation. LM.custom &lt;- function(w0, data, run) { w &lt;- w.previous &lt;- w0 k &lt;- 0 feval &lt;- 0 mu &lt;- 1e-2 ftol &lt;- 1e-6 ptol &lt;- 1e-6 gtol &lt;- 0 maxiter &lt;- 200 maxfev &lt;- 600 y &lt;- data$od[data$run == run] x_data &lt;- unique(data$conc) n &lt;- length(x_data) x_rep &lt;- rep(x_data, 2) df_temp &lt;- data.frame(x = unique(data$conc), od = y) f &lt;- function(w0) { model &lt;- w0[1] + (w0[4] - w0[1]) / (1 + (x_rep / w0[3])^w0[2])^w0[5] return(model) } model.fn &lt;- function(x, a, b, c, d, e) { a + (d - a) / (1 + (x / c)^b)^e } chi.squre.fn &lt;- function(w) { chi.sq &lt;- sum((y - f(w))^2) return(chi.sq) } norm.fn &lt;- function(x) sqrt(sum(x^2)) initial.chisq &lt;- previous.chisq &lt;- chi.squre.fn(w) calculate.cosine.angles &lt;- function(y, w) { r &lt;- y - f(w) J &lt;- numDeriv::jacobian(f, w) norm.r &lt;- sqrt(sum(r^2)) cosine.angles &lt;- numeric(ncol(J)) for (i in 1:ncol(J)) { J.col &lt;- J[, i] norm.J.col &lt;- sqrt(sum(J.col^2)) dot.product &lt;- sum(J.col * r) cosine.angles[i] &lt;- dot.product / (norm.J.col * norm.r) } return(cosine.angles) } while (k &lt; maxiter &amp;&amp; feval &lt; maxfev) { J &lt;- numDeriv::jacobian(f, w) r1 &lt;- y - f(w) # Jr &lt;- t(J) %*% r1 feval &lt;- feval + 1 if (feval == maxfev) { cat(&quot;Number of function evaluations reached: &quot;, feval, &quot;\\n&quot;) break } JTJ &lt;- t(J) %*% J + mu * diag(ncol(J)) QR &lt;- qr(JTJ) damp &lt;- mu * diag(ncol(JTJ)) JTJ.inv &lt;- tryCatch({ qr.solve(JTJ) }, error = function(e) { ginv(JTJ) }) dk &lt;- JTJ.inv %*% Jr w1 &lt;- w + dk chisq.new &lt;- chi.squre.fn(w1) actual.relative.reduction &lt;- abs( previous.chisq - chisq.new) / previous.chisq predicted.relative.reduction &lt;- p &lt;- (previous.chisq - chisq.new) / abs(t(dk) %*% (damp %*% dk + Jr)) relative.error &lt;- max(abs(w1 - w.previous)) / max(abs(w.previous)) cosine.angles &lt;- calculate.cosine.angles(y, w) convergence &lt;- function() { if (actual.relative.reduction &lt; ftol &amp; p &lt; ftol) { convergence.result &lt;- paste(&quot;ARD:&quot;, actual.relative.reduction, &quot;\\n&quot;, &quot;PRD:&quot;, p ) } else if (relative.error &lt; ptol) { convergence.result &lt;- paste(&quot;RE:&quot;, relative.error) } else { convergence.result &lt;- paste(&quot;CA:&quot;, cosine.angles) } } convergence.result &lt;- c() if (actual.relative.reduction &lt; ftol &amp; p &lt; ftol || relative.error &lt; ptol || max(abs(cosine.angles)) &lt; gtol ) { convergence.result &lt;- convergence() break } if (p &gt; 0) { mu &lt;- max(mu / 9, 1e-10) w.previous &lt;- w w &lt;- w1 } else { mu &lt;- min(mu * 11, 1e10) } previous.chisq &lt;- chisq.new k &lt;- k + 1 if (k == maxiter) { cat(&quot;Number of iterations till stop: &quot;, k, &quot;\\n&quot;) break } } result &lt;- list(parameters = w) return(result) } result.list.mdf &lt;- list() for(i in unique(df_temp$run)){ w &lt;- c(df_start$Max[df_start$run==i], df_start$Slope[df_start$run==i], df_start$C[df_start$run==i], df_start$Min[df_start$run==i], df_start$M[df_start$run==i]) result.list.mdf[[i]] &lt;- try(LM.custom(w, df_temp, i)) } ## Number of iterations till stop: 200 ## Number of iterations till stop: 200 ## Number of iterations till stop: 200 ## Number of iterations till stop: 200 ## Number of iterations till stop: 200 ## Number of iterations till stop: 200 ## Number of iterations till stop: 200 ## Number of iterations till stop: 200 result.list.mdf &lt;- result.list.mdf[lapply(result.list.mdf,length)&gt;0] result.list.mdf &lt;- as.vector(unlist(result.list.mdf)) result.list.mdf &lt;- matrix(result.list.mdf, ncol=5, byrow = TRUE) parameters &lt;- as.data.frame(result.list.mdf) parameters$run &lt;- c(1:101) colnames(parameters) &lt;- c(&quot;w1&quot;, &quot;w2&quot;, &quot;w3&quot;, &quot;w4&quot;, &quot;w5&quot;, &quot;run&quot;) This code is for nlsLM in R to do the same. LM.nlsLM &lt;- function(w0, data, run) { y &lt;- data$od[data$run == run] x_data &lt;- unique(data$conc) n &lt;- length(x_data) x_rep &lt;- rep(x_data, 2) y &lt;- data$od[data$run == run] df_temp &lt;- data.frame(x=unique(data$conc), od=y) model_fn &lt;- function(x, a, b, c, d, e) { a + (d - a) / (1 + (x / c)^b)^e } fit &lt;- try(nlsLM(y ~ model_fn(x, a, b, c, d, e), data = df_temp, start = list(a = w0[1], b = w0[2], c = w0[3], d = w0[4], e = w0[5]), control = nls.lm.control(maxiter = 1000, ftol = 1e-6))) result &lt;- coef(fit) } result.list.nlsLM &lt;- list() for(i in unique(df_temp$run)){ w &lt;- c(df_start$Max[df_start$run==i], df_start$Slope[df_start$run==i], df_start$C[df_start$run==i], df_start$Min[df_start$run==i], df_start$M[df_start$run==i]) result.list.nlsLM[[i]] &lt;- try(LM.nlsLM(w, df_temp, i)) } result.list.nlsLM &lt;- result.list.nlsLM[lapply(result.list.nlsLM,length)&gt;0] result.list.nlsLM &lt;- matrix(unlist(result.list.nlsLM), ncol=5, byrow = TRUE) parameters.2 &lt;- as.data.frame(result.list.nlsLM) parameters.2$run &lt;- c(1:101) colnames(parameters.2) &lt;- c(&quot;w1&quot;, &quot;w2&quot;, &quot;w3&quot;, &quot;w4&quot;, &quot;w5&quot;, &quot;run&quot;) Next, using those parameters and assay signal, we’ll back-calculate the concentrations of all calibration standard samples. For this, we need the inverse function of the 5 PL curve model: \\[ \\text{Concentration} = C \\cdot \\left( \\left( \\frac{A - D}{od - D} \\right)^{\\frac{1}{G}} - 1 \\right)^{\\frac{1}{B}} \\] Where, D: Infinite X asymptote, W1 B: Slope/Hill, W2 C: Inflection point, W3 A: Small X asymptote, W4 G: asymmetric factor, W5 df_temp[, &quot;xhat1&quot;] &lt;- NA ## for my code df_temp[, &quot;xhat2&quot;] &lt;- NA ## for nlsLM x.hat &lt;- function(od, w){ xhat &lt;- w[3] * (((w[4] - w[1])/(od - w[1]))^(1/w[5]) - 1)^(1 / w[2]) return(xhat) } for(i in 1:nrow(df_temp)){ od &lt;- df_temp[i, 4] w &lt;- as.vector(unlist(parameters[parameters$run == df_temp[i, 5], c(1:5)])) df_temp[i, 6] &lt;- x.hat(od, w) } for(i in 1:nrow(df_temp)){ od &lt;- df_temp[i, 4] w &lt;- as.vector(unlist(parameters.2[parameters.2$run == df_temp[i, 5], c(1:5)])) df_temp[i, 7] &lt;- x.hat(od, w) } df_temp &lt;- df_temp %&gt;% mutate(bias1 = round((xhat1 - conc)/conc * 100, 2), bias2 = round((xhat2 - conc)/conc * 100, 2)) To confirm the comparison results, we reconstruct the %Bias data as follows: bias.1 &lt;- df_temp[, c(1:6, 8)] bias.2 &lt;- df_temp[, c(1:5, 7, 9)] colnames(bias.1) &lt;- c(&quot;calb&quot;, &quot;conc&quot;, &quot;rep&quot;, &quot;od&quot;, &quot;run&quot;, &quot;xhat&quot;, &quot;bias&quot;) colnames(bias.2) &lt;- c(&quot;calb&quot;, &quot;conc&quot;, &quot;rep&quot;, &quot;od&quot;, &quot;run&quot;, &quot;xhat&quot;, &quot;bias&quot;) bias &lt;- rbind(bias.1, bias.2) bias$code &lt;- gl(2, nrow(bias)/2, labels = c(&quot;My code&quot;, &quot;nlsLM&quot;)) Finally, we visualize the %Bias of all calibration standard samples as follows: set.seed(03) g1 &lt;- ggplot(bias, aes(log(conc), bias, col=code)) + geom_jitter(alpha=.2)+ scale_colour_manual(values = c(&quot;#1226A3&quot;, &quot;#FF0000&quot;)) + scale_x_continuous(breaks = c(log(5), log(10), log(15), log(30), log(60), log(80), log(100), log(150), log(200), log(400), log(800), log(1600)), labels = c(&quot;5&quot;, &quot;10&quot;, &quot;15&quot;, &quot;30&quot;, &quot;60&quot;, &quot;80&quot;, &quot;100&quot;, &quot;150&quot;, &quot;200&quot;, &quot;400&quot;, &quot;800&quot;, &quot;1600&quot;)) + theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(), panel.background = element_blank(), axis.line = element_line(colour = &quot;black&quot;)) + theme(axis.text.x = element_text(angle = 45, hjust = 1)) + ylab(&quot;Bias(%)&quot;)+ xlab(&quot;Concentration (ng/mL) in log sacle&quot;) ggsave(here(&quot;result&quot;,&quot;g1.png&quot;), plot=g1, dpi=300) ## Saving 7 x 7 in image Most of the points (the %Bias) overlap closely, indicating that the results are very similar. Only a few points show significant vertical differences, even considering the use of geom_jitter1, but these are minor and do not affect the overall conclusion. If we want to compare the parameters estimation results and %Bias results of a specific run, first specify a run we want as follows: n &lt;- 2 And do as follows: p1 &lt;- unlist(list(parameters[parameters$run ==n, c(1:5)])) p2 &lt;- unlist(list(parameters.2[parameters.2$run ==n,c(1:5) ])) b1 &lt;- unlist(list(bias.1$bias[bias.1$run == n])) b2 &lt;- unlist(list(bias.2$bias[bias.2$run == n])) cat(&quot;The custom code parameter result:&quot; , p1, &quot;\\n&quot;) cat(&quot;The nlsLM in R parameter result:&quot; , p2, &quot;\\n&quot;) cat(&quot;The custom code %Bias result:&quot; , b1, &quot;\\n&quot;) cat(&quot;The nlsLM in R %Bias result:&quot; , b2) And the result will be shown up: ## The custom code parameter result: 6.589187 1.323836 193.6181 0.01556511 0.237396 ## The nlsLM in R parameter result: 6.58917 1.323835 193.6187 0.01556504 0.2373973 ## The custom code %Bias result: 73.07 -17.99 -5.66 -0.6 3.78 1.47 2.19 2.33 -0.67 -2.17 7.4 -6.25 55.9 -24.51 -9.5 1.44 -3.48 -5.19 2 -2.22 0.38 1.47 -6.24 6.54 ## The nlsLM in R %Bias result: 73.07 -17.99 -5.66 -0.6 3.78 1.47 2.19 2.33 -0.67 -2.17 7.4 -6.25 55.9 -24.51 -9.5 1.44 -3.48 -5.19 2 -2.22 0.38 1.47 -6.24 6.54 For visualization of the %Bias, do as follows (the grey area indicates quantification range and acceptable range of the %Bias) : g2 &lt;- ggplot(bias[bias$run==n,], aes(log(conc), bias, col=code)) + geom_jitter(alpha=.2)+ scale_colour_manual(values = c(&quot;#1226A3&quot;, &quot;#FF0000&quot;)) + scale_x_continuous(breaks = c(log(5), log(10), log(15), log(30), log(60), log(80), log(100), log(150), log(200), log(400), log(800), log(1600)), labels = c(&quot;5&quot;, &quot;10&quot;, &quot;15&quot;, &quot;30&quot;, &quot;60&quot;, &quot;80&quot;, &quot;100&quot;, &quot;150&quot;, &quot;200&quot;, &quot;400&quot;, &quot;800&quot;, &quot;1600&quot;)) + theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(), panel.background = element_blank(), axis.line = element_line(color = &quot;black&quot;)) + theme(axis.text.x = element_text(angle = 45, hjust = 1)) + geom_rect(aes(xmin=log(12), xmax=log(250), ymin=-20, ymax=20), fill= &quot;grey&quot;, alpha =.01, color=NA)+ ylab(&quot;Bias(%)&quot;)+ xlab(&quot;Concentration (ng/mL) in log sacle&quot;) ggsave(here(&quot;result&quot;, &quot;g2.png&quot;), plot= g2, dpi=300) ## Saving 7 x 7 in image Here’s a another result: n &lt;- 74 p1 &lt;- unlist(list(parameters[parameters$run ==n, c(1:5)])) p2 &lt;- unlist(list(parameters.2[parameters.2$run ==n,c(1:5) ])) b1 &lt;- unlist(list(bias.1$bias[bias.1$run == n])) b2 &lt;- unlist(list(bias.2$bias[bias.2$run == n])) cat(&quot;The custom code parameter result:&quot; , p1, &quot;\\n&quot;) cat(&quot;The nlsLM in R parameter result:&quot; , p2, &quot;\\n&quot;) cat(&quot;The custom code %Bias result:&quot; , b1, &quot;\\n&quot;) cat(&quot;The nlsLM in R %Bias result:&quot; , b2) ## The custom code parameter result: 5.491253 1.046752 273.3658 -0.01495246 0.2559358 ## The nlsLM in R parameter result: 6.812359 1.078459 224.6647 -0.01205867 0.1744507 ## The custom code %Bias result: 40.01 9.35 -6.36 -3.81 -1.6 -4.69 -1.8 1.26 -1.02 -5.45 5.88 4.28 36.82 3.68 -9.85 -12.56 -0.04 1.89 -1.03 10.15 3.52 -2.94 -1.32 -5.02 ## The nlsLM in R %Bias result: 36.4 9.05 -5.99 -2.93 -1.15 -4.46 -1.76 1.05 -1.28 -5.56 6.16 4.11 33.06 3.19 -9.56 -11.75 0.4 2.07 -1 9.9 3.24 -3.04 -1.07 -5.03 g3 &lt;- ggplot(bias[bias$run==n,], aes(log(conc), bias, col=code)) + geom_jitter(alpha=.2)+ scale_colour_manual(values = c(&quot;#1226A3&quot;, &quot;#FF0000&quot;)) + scale_x_continuous(breaks = c(log(5), log(10), log(15), log(30), log(60), log(80), log(100), log(150), log(200), log(400), log(800), log(1600)), labels = c(&quot;5&quot;, &quot;10&quot;, &quot;15&quot;, &quot;30&quot;, &quot;60&quot;, &quot;80&quot;, &quot;100&quot;, &quot;150&quot;, &quot;200&quot;, &quot;400&quot;, &quot;800&quot;, &quot;1600&quot;)) + theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(), panel.background = element_blank(), axis.line = element_line(color = &quot;black&quot;)) + theme(axis.text.x = element_text(angle = 45, hjust = 1)) + geom_rect(aes(xmin=log(12), xmax=log(250), ymin=-20, ymax=20), fill= &quot;grey&quot;, alpha =.01, color=NA)+ ylab(&quot;Bias(%)&quot;)+ xlab(&quot;Concentration (ng/mL) in log sacle&quot;) ggsave(here(&quot;result&quot;,&quot;g3.png&quot;),plot=g3, dpi=300) ## Saving 7 x 7 in image The parameter results were slightly different and so were the %Bias results. However, the discrepancy between the %Bias results were small and the %Bias of all calibration standard curve samples within the quantification range (from LLOQ to ULOQ) met the acceptance criteria for accuracy. Finally, this is the last example of comparison of the %Bias results. The new %Bias results are obtained by Watson LIMS. Watson LIMS also provides a wide range of algorithm for non-linear regression and for the optimization of parameters and computation of %Bias, I chosen Marquardt algorithm that is the LM algorithm. As can be seen below, most of points overlap closely. The geom_jitter function is used to add a small random variation to the position of data points, shifting them both vertically and horizontally. This helps preventing the data from overlapping when plotted, making them more distinguishable, especially when multiple data points have the same or very similar values.↩︎ "],["closing-remarks.html", "Closing Remarks", " Closing Remarks In my experience in bioanalytical research, particularly when working with complex data from ligand binding assays or other experimental methods, I’ve found that having a solid understanding of the mathematical principles behind the tools we use is absolutely crucial. However, I’ve noticed that many bioanalytical scientists, including myself in the past, often rely on established functions or commercial software without fully grasping the underlying mathematical details of the algorithms, simply because there aren’t enough opportunities to learn these principles. This, I’ve realized, can limit our ability and creativity. One of the key goals of my work has been to provide a clearer, more transparent understanding of the mathematical principles involved in non-linear regression. I’ve found that this kind of transparency not only enhances my own understanding but also benefits others who share similar experiences. I hope my work allows for a deeper appreciation of the process, which can ultimately lead to significant improvements in the quality of our work. "],["references.html", "References", " References The Levenberg-Marquardt algorithm for nonlinear least squares curve-fitting problems https://people.duke.edu/~hpgavin/lm.pdf Methods for non-linear least squares problems (https://www2.imm.dtu.dk/pubdb/edoc/imm3215.pdf) minpack.lm https://github.com/cran/minpack.lm nlsLM (https://cran.r-project.org/web/packages/minpack.lm/minpack.lm.pdf) Numerical Recipes Routines and Examples in Basic, Julein C.Sprott "]]
