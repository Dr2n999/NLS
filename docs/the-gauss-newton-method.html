<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 2 The Gauss-Newton Method | Non-linear regression</title>
  <meta name="description" content="<p>This is a minimal example of using the bookdown package to write a book.
set in the _output.yml file.
The HTML output format for this example is bookdown::gitbook,</p>" />
  <meta name="generator" content="bookdown 0.42 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 2 The Gauss-Newton Method | Non-linear regression" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="<p>This is a minimal example of using the bookdown package to write a book.
set in the _output.yml file.
The HTML output format for this example is bookdown::gitbook,</p>" />
  <meta name="github-repo" content="rstudio/bookdown-demo" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 2 The Gauss-Newton Method | Non-linear regression" />
  
  <meta name="twitter:description" content="<p>This is a minimal example of using the bookdown package to write a book.
set in the _output.yml file.
The HTML output format for this example is bookdown::gitbook,</p>" />
  

<meta name="author" content="Kyungmin In, GCCL" />


<meta name="date" content="2025-04-01" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="the-gradient-descent-method.html"/>
<link rel="next" href="levenberg-marquardt-lm-algorithm.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>
<link href="libs/htmltools-fill-0.5.8.1/fill.css" rel="stylesheet" />
<script src="libs/htmlwidgets-1.6.4/htmlwidgets.js"></script>
<script src="libs/plotly-binding-4.10.4/plotly.js"></script>
<script src="libs/typedarray-0.1/typedarray.min.js"></script>
<link href="libs/crosstalk-1.2.1/css/crosstalk.min.css" rel="stylesheet" />
<script src="libs/crosstalk-1.2.1/js/crosstalk.min.js"></script>
<link href="libs/plotly-htmlwidgets-css-2.11.1/plotly-htmlwidgets.css" rel="stylesheet" />
<script src="libs/plotly-main-2.11.1/plotly-latest.min.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Non-linear regression</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Non-Linear Regression and Optimization Methods in LBA Data Analysis</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#key-optimization-methods-in-non-linear-regression"><i class="fa fa-check"></i>Key Optimization Methods in Non-Linear Regression</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#the-levenberg-marquardt-algorithm-a-hybrid-approach"><i class="fa fa-check"></i>The Levenberg-Marquardt Algorithm: A Hybrid Approach</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#why-should-we-care-about-these-optimization-methods"><i class="fa fa-check"></i>Why Should We Care About These Optimization Methods?</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#practical-implementation-in-r"><i class="fa fa-check"></i>Practical Implementation in R</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="author.html"><a href="author.html"><i class="fa fa-check"></i>Author</a></li>
<li class="chapter" data-level="1" data-path="the-gradient-descent-method.html"><a href="the-gradient-descent-method.html"><i class="fa fa-check"></i><b>1</b> The Gradient descent method</a>
<ul>
<li class="chapter" data-level="1.1" data-path="the-gradient-descent-method.html"><a href="the-gradient-descent-method.html#computation-of-the-chi-squared-function"><i class="fa fa-check"></i><b>1.1</b> Computation of the chi-squared function</a></li>
<li class="chapter" data-level="1.2" data-path="the-gradient-descent-method.html"><a href="the-gradient-descent-method.html#computation-of-the-gradient"><i class="fa fa-check"></i><b>1.2</b> Computation of the gradient</a></li>
<li class="chapter" data-level="1.3" data-path="the-gradient-descent-method.html"><a href="the-gradient-descent-method.html#the-parameter-updating"><i class="fa fa-check"></i><b>1.3</b> The parameter updating</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="the-gauss-newton-method.html"><a href="the-gauss-newton-method.html"><i class="fa fa-check"></i><b>2</b> The Gauss-Newton Method</a>
<ul>
<li class="chapter" data-level="2.1" data-path="the-gauss-newton-method.html"><a href="the-gauss-newton-method.html#computation-of-the-chi-squared-function-1"><i class="fa fa-check"></i><b>2.1</b> Computation of the chi-squared function</a></li>
<li class="chapter" data-level="2.2" data-path="the-gauss-newton-method.html"><a href="the-gauss-newton-method.html#computation-of-the-gradient-1"><i class="fa fa-check"></i><b>2.2</b> Computation of the gradient</a></li>
<li class="chapter" data-level="2.3" data-path="the-gauss-newton-method.html"><a href="the-gauss-newton-method.html#the-parameter-updating-1"><i class="fa fa-check"></i><b>2.3</b> The parameter updating</a></li>
<li class="chapter" data-level="2.4" data-path="the-gauss-newton-method.html"><a href="the-gauss-newton-method.html#summary-and-comparison-to-gradient-descent"><i class="fa fa-check"></i><b>2.4</b> Summary and Comparison to Gradient Descent</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="levenberg-marquardt-lm-algorithm.html"><a href="levenberg-marquardt-lm-algorithm.html"><i class="fa fa-check"></i><b>3</b> Levenberg-Marquardt (LM) algorithm</a>
<ul>
<li class="chapter" data-level="3.1" data-path="levenberg-marquardt-lm-algorithm.html"><a href="levenberg-marquardt-lm-algorithm.html#the-damping-factor-lambda"><i class="fa fa-check"></i><b>3.1</b> The Damping Factor <span class="math inline">\(\lambda\)</span></a></li>
<li class="chapter" data-level="3.2" data-path="levenberg-marquardt-lm-algorithm.html"><a href="levenberg-marquardt-lm-algorithm.html#advantages-of-the-lm-algorithm"><i class="fa fa-check"></i><b>3.2</b> Advantages of the LM Algorithm</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="applying-the-lm-algorithm-to-a-damped-oscillation-model.html"><a href="applying-the-lm-algorithm-to-a-damped-oscillation-model.html"><i class="fa fa-check"></i><b>4</b> Applying the LM Algorithm to a Damped Oscillation Model</a>
<ul>
<li class="chapter" data-level="4.1" data-path="applying-the-lm-algorithm-to-a-damped-oscillation-model.html"><a href="applying-the-lm-algorithm-to-a-damped-oscillation-model.html#example-data"><i class="fa fa-check"></i><b>4.1</b> Example Data</a></li>
<li class="chapter" data-level="4.2" data-path="applying-the-lm-algorithm-to-a-damped-oscillation-model.html"><a href="applying-the-lm-algorithm-to-a-damped-oscillation-model.html#data-fitting-using-nlslm"><i class="fa fa-check"></i><b>4.2</b> Data fitting using nlsLM</a></li>
<li class="chapter" data-level="4.3" data-path="applying-the-lm-algorithm-to-a-damped-oscillation-model.html"><a href="applying-the-lm-algorithm-to-a-damped-oscillation-model.html#how-the-algorithm-works"><i class="fa fa-check"></i><b>4.3</b> How the Algorithm Works</a></li>
<li class="chapter" data-level="4.4" data-path="applying-the-lm-algorithm-to-a-damped-oscillation-model.html"><a href="applying-the-lm-algorithm-to-a-damped-oscillation-model.html#custom-implementation-of-the-algorithm"><i class="fa fa-check"></i><b>4.4</b> Custom Implementation of the Algorithm</a></li>
<li class="chapter" data-level="4.5" data-path="applying-the-lm-algorithm-to-a-damped-oscillation-model.html"><a href="applying-the-lm-algorithm-to-a-damped-oscillation-model.html#convergence-criteria-of-the-algorithm"><i class="fa fa-check"></i><b>4.5</b> Convergence Criteria of the Algorithm</a></li>
<li class="chapter" data-level="4.6" data-path="applying-the-lm-algorithm-to-a-damped-oscillation-model.html"><a href="applying-the-lm-algorithm-to-a-damped-oscillation-model.html#code-example"><i class="fa fa-check"></i><b>4.6</b> Code Example</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="applying-the-algorithm-to-a-sphere-function-model.html"><a href="applying-the-algorithm-to-a-sphere-function-model.html"><i class="fa fa-check"></i><b>5</b> Applying the Algorithm to a Sphere function model</a>
<ul>
<li class="chapter" data-level="5.1" data-path="applying-the-algorithm-to-a-sphere-function-model.html"><a href="applying-the-algorithm-to-a-sphere-function-model.html#code-explanation"><i class="fa fa-check"></i><b>5.1</b> Code Explanation</a></li>
<li class="chapter" data-level="5.2" data-path="applying-the-algorithm-to-a-sphere-function-model.html"><a href="applying-the-algorithm-to-a-sphere-function-model.html#example-data-1"><i class="fa fa-check"></i><b>5.2</b> Example data</a></li>
<li class="chapter" data-level="5.3" data-path="applying-the-algorithm-to-a-sphere-function-model.html"><a href="applying-the-algorithm-to-a-sphere-function-model.html#optimization-with-the-algorithm"><i class="fa fa-check"></i><b>5.3</b> Optimization with the Algorithm</a></li>
<li class="chapter" data-level="5.4" data-path="applying-the-algorithm-to-a-sphere-function-model.html"><a href="applying-the-algorithm-to-a-sphere-function-model.html#applying-the-algorithm"><i class="fa fa-check"></i><b>5.4</b> Applying the Algorithm</a></li>
<li class="chapter" data-level="5.5" data-path="applying-the-algorithm-to-a-sphere-function-model.html"><a href="applying-the-algorithm-to-a-sphere-function-model.html#code-example-1"><i class="fa fa-check"></i><b>5.5</b> Code Example</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="applying-the-lm-algorithm-to-lba-data.html"><a href="applying-the-lm-algorithm-to-lba-data.html"><i class="fa fa-check"></i><b>6</b> Applying the LM Algorithm to LBA data</a>
<ul>
<li class="chapter" data-level="6.1" data-path="applying-the-lm-algorithm-to-lba-data.html"><a href="applying-the-lm-algorithm-to-lba-data.html#pl-curve"><i class="fa fa-check"></i><b>6.1</b> 5 PL curve</a></li>
<li class="chapter" data-level="6.2" data-path="applying-the-lm-algorithm-to-lba-data.html"><a href="applying-the-lm-algorithm-to-lba-data.html#data-set"><i class="fa fa-check"></i><b>6.2</b> Data set</a></li>
<li class="chapter" data-level="6.3" data-path="applying-the-lm-algorithm-to-lba-data.html"><a href="applying-the-lm-algorithm-to-lba-data.html#initial-parameter-estimation"><i class="fa fa-check"></i><b>6.3</b> Initial parameter estimation</a></li>
<li class="chapter" data-level="6.4" data-path="applying-the-lm-algorithm-to-lba-data.html"><a href="applying-the-lm-algorithm-to-lba-data.html#example-of-code"><i class="fa fa-check"></i><b>6.4</b> Example of Code</a></li>
<li class="chapter" data-level="6.5" data-path="applying-the-lm-algorithm-to-lba-data.html"><a href="applying-the-lm-algorithm-to-lba-data.html#visualization-of-the-fitting-process"><i class="fa fa-check"></i><b>6.5</b> Visualization of the fitting process</a></li>
<li class="chapter" data-level="6.6" data-path="applying-the-lm-algorithm-to-lba-data.html"><a href="applying-the-lm-algorithm-to-lba-data.html#result"><i class="fa fa-check"></i><b>6.6</b> Result</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="evaluation-of-the-custom-code.html"><a href="evaluation-of-the-custom-code.html"><i class="fa fa-check"></i><b>7</b> Evaluation of the custom code</a></li>
<li class="chapter" data-level="" data-path="closing-remarks.html"><a href="closing-remarks.html"><i class="fa fa-check"></i>Closing Remarks</a></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Non-linear regression</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="the-gauss-newton-method" class="section level1 hasAnchor" number="2">
<h1><span class="header-section-number">Chapter 2</span> The Gauss-Newton Method<a href="the-gauss-newton-method.html#the-gauss-newton-method" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>The <strong>Gauss-Newton method</strong> is a second-order optimization technique used in non-linear regression to minimize the <strong>chi-squared cost function</strong>. It is especially useful for <strong>non-linear least squares problems</strong> where the model’s prediction depends on parameters in a nonlinear way.</p>
<p>The key here is that the <strong>Gauss-Newton method</strong> assumes the cost function behaves approximately <strong>linearly</strong> around the current parameter <span class="math inline">\(\alpha\)</span>. This means we can approximate the change in the cost function <span class="math inline">\(\chi^2\)</span> when <span class="math inline">\(\alpha\)</span> is perturbed by a small amount <span class="math inline">\(h\)</span>.</p>
<p><strong>Gradient descent</strong>, on the other hand, is a first-order optimization method that uses only the gradient (the first derivative) of the cost function to update the parameters.</p>
<div id="computation-of-the-chi-squared-function-1" class="section level2 hasAnchor" number="2.1">
<h2><span class="header-section-number">2.1</span> Computation of the chi-squared function<a href="the-gauss-newton-method.html#computation-of-the-chi-squared-function-1" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>We can approximate the chi-squared function for a small change in <span class="math inline">\(\alpha\)</span>:<br />
<span class="math display">\[
\chi^2(\alpha + h) = \chi^2(\alpha) + \frac{\partial \chi^2}{\partial \alpha} h
\]</span></p>
<p>This is a <strong>first-order Taylor expansion</strong> of the cost function, assuming that the change in <span class="math inline">\(\alpha\)</span> is small enough that the higher-order terms are negligible. The term <span class="math inline">\(\frac{\partial \chi^2}{\partial \alpha}\)</span> represents the <strong>gradient</strong> of the chi-squared function with respect to <span class="math inline">\(\alpha\)</span>, which tells us how the cost changes as we move in the direction of <span class="math inline">\(h\)</span>.</p>
<p>Let’s start with the <strong>chi-squared function</strong>:</p>
<p><span class="math display">\[
\chi^2(\alpha) = y^T W y - 2 y^T W \hat{y}(\alpha) + \hat{y}(\alpha)^T W \hat{y}(\alpha)
\]</span></p>
<p>Where:<br />
- <span class="math inline">\(y\)</span> is the observed data,<br />
- <span class="math inline">\(\hat{y}(\alpha)\)</span> is the predicted value from the model for the current parameter <span class="math inline">\(\alpha\)</span>,<br />
- <span class="math inline">\(W\)</span> is a weight matrix.</p>
<p>We assume that the chi-squared function around the current parameter <span class="math inline">\(\alpha\)</span> is linear and that can be expressed as:</p>
<p><span class="math display">\[
\chi^2(\alpha +h)\approx\chi^2(\alpha) + \frac{\partial \chi^2}{\partial \alpha}h
\]</span></p>
<p>This is a <strong>first-order Taylor expansion</strong> of the cost function, assuming that the change in <span class="math inline">\(\alpha\)</span> is small enough that the higher-order terms are negligible. The term <span class="math inline">\(\frac{\partial \chi^2}{\partial \alpha}\)</span> represents the <strong>gradient</strong> of the chi-squared function with respect to <span class="math inline">\(\alpha\)</span>, which tells us how the cost changes as we move in the direction of <span class="math inline">\(h\)</span>.</p>
<p>Now, we can substitute the expression for <span class="math inline">\(\hat{y}(\alpha + h)\)</span> into the cost function. Using the first-order approximation for <span class="math inline">\(\hat{y}(\alpha + h)\)</span>, which is:</p>
<p><span class="math display">\[
\hat{y}(\alpha + h)\approx \hat{y}(\alpha) + J h
\]</span></p>
<p>Where <span class="math inline">\(J\)</span> is the <strong>Jacobian matrix</strong>, which contains the partial derivatives of the model predictions with respect to the parameters <span class="math inline">\(\alpha\)</span>, we substitute this into the expanded chi-squared function:</p>
<p><span class="math display">\[
\chi^2(\alpha + h) \approx y^T W y - 2 y^T W (\hat{y}(\alpha) + J h) + (\hat{y}(\alpha) + J h)^T W (\hat{y}(\alpha) + J h)
\]</span></p>
<p>This gives the following terms:</p>
<ul>
<li>The first term, <span class="math inline">\(y^T W y\)</span>, is constant and doesn’t change with <span class="math inline">\(\alpha\)</span>.</li>
<li>The second term expands as:</li>
</ul>
<p><span class="math display">\[
-2 y^T W (\hat{y}(\alpha) + J h) = -2 y^T W \hat{y}(\alpha) - 2 y^T W J h
\]</span></p>
<ul>
<li>The third term is:</li>
</ul>
<p><span class="math display">\[
(\hat{y}(\alpha) + J h)^T W (\hat{y}(\alpha) + J h) = \hat{y}(\alpha)^T W \hat{y}(\alpha) + \hat{y}(\alpha)^T W J h + h^T J^T W \hat{y}(\alpha) + h^T J^T W J h
\]</span></p>
<p>We can simplify the expression further since:</p>
<ul>
<li>The terms <span class="math inline">\(y^T W y\)</span> and <span class="math inline">\(\hat{y}(\alpha)^T W \hat{y}(\alpha)\)</span> remain unchanged.</li>
<li>The cross terms <span class="math inline">\(\hat{y}(\alpha)^T W J h\)</span> and <span class="math inline">\(h^T J^T W \hat{y}(\alpha)\)</span> are equal, so they combine into:</li>
</ul>
<p><span class="math display">\[
2 \hat{y}(\alpha)^T W J h
\]</span></p>
<p>Thus, the simplified expression for <span class="math inline">\(\chi^2(\alpha + h)\)</span> is:</p>
<p><span class="math display">\[
\chi^2(\alpha + h) \approx y^T W y - 2 y^T W \hat{y}(\alpha) - 2 y^T W J h + \hat{y}(\alpha)^T W \hat{y}(\alpha) + 2 \hat{y}(\alpha)^T W J h + h^T J^T W J h
\]</span></p>
<p>This is the <strong>expanded chi-squared function</strong> for a small perturbation <span class="math inline">\(h\)</span>.</p>
</div>
<div id="computation-of-the-gradient-1" class="section level2 hasAnchor" number="2.2">
<h2><span class="header-section-number">2.2</span> Computation of the gradient<a href="the-gauss-newton-method.html#computation-of-the-gradient-1" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Let’s move on to the next step where we take the derivative of the expanded form of <span class="math inline">\(\chi^2(\alpha + h)\)</span> with respect to <span class="math inline">\(h\)</span>.</p>
<ul>
<li>We have already expanded <span class="math inline">\(\chi^2(\alpha + h)\)</span> as:</li>
</ul>
<p><span class="math display">\[
\chi^2(\alpha + h) \approx y^T W y + \hat{y}(\alpha)^T W \hat{y}(\alpha) - 2 y^T W \hat{y}(\alpha) - 2 (y - \hat{y}(\alpha))^T W J h + h^T J^T W J h
\]</span></p>
<ul>
<li><p>so let’s compute the derivative of <span class="math inline">\(\chi^2(\alpha + h)\)</span> with respect to the update vector <span class="math inline">\(h\)</span>.</p></li>
<li><p>The first section <span class="math inline">\(y^T W y\)</span>, its derivative is 0.</p></li>
<li><p>The second section <span class="math inline">\(\hat{y}(\alpha)^T W \hat{y}(\alpha)\)</span>, its derivative is also 0.</p></li>
<li><p>The third section -<span class="math inline">\(2 y^T W \hat{y}(\alpha)\)</span> is also 0.</p></li>
<li><p>The fourth section -<span class="math inline">\(2 (y - \hat{y}(\alpha))^T W J h\)</span>:</p></li>
</ul>
<p><span class="math display">\[
\frac{\partial}{\partial h} \left( -2 (y - \hat{y}(\alpha))^T W J h \right) = -2 W^T J (y - \hat{y}(\alpha))
\]</span></p>
<ul>
<li>For the last section <span class="math inline">\(h^T J^T W J h\)</span>:</li>
</ul>
<p><span class="math display">\[
\frac{\partial}{\partial h} (h^T J^T W J h) = 2 J^T W J h
\]</span></p>
<ul>
<li>Combining all the terms, we can get the derivative of <span class="math inline">\(\chi^2(\alpha + h)\)</span> with respect to <span class="math inline">\(h\)</span>:</li>
</ul>
<p><span class="math display">\[
\frac{\partial \chi^2(\alpha + h)}{\partial h} \approx -2 W^T J (y - \hat{y}(\alpha)) + 2 J^T W J h
\]</span></p>
<ul>
<li>It is also commonly known as:</li>
</ul>
<p><span class="math display">\[
\frac{\partial \chi^2(\alpha + h)}{\partial h} \approx -2 (y - \hat{y}(\alpha))^T W J + 2 h^T J^T W J
\]</span></p>
<p>where the <span class="math inline">\(J^TWJ\)</span> is also known as the approximation of Hessian matrix.</p>
</div>
<div id="the-parameter-updating-1" class="section level2 hasAnchor" number="2.3">
<h2><span class="header-section-number">2.3</span> The parameter updating<a href="the-gauss-newton-method.html#the-parameter-updating-1" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>In the previous steps, we derived the gradient of the cost function <span class="math inline">\(\chi^2(\alpha + h)\)</span> with respect to the update direction <span class="math inline">\(h\)</span>. This gradient provides insight into how the cost function changes as the parameters <span class="math inline">\(\alpha\)</span> are adjusted by a small amount <span class="math inline">\(h\)</span>.</p>
<p>Here’s the expression for the gradient:</p>
<p><span class="math display">\[
\frac{\partial \chi^2(\alpha + h)}{\partial h} \approx -2 \left( (y - \hat{y}(\alpha))^T W J + 2 h^T J^T W J \right)
\]</span></p>
<p>The goal is to find the update direction <span class="math inline">\(h_{\text{gn}}\)</span> that minimizes the cost function. To do this, we set this derivative equal to zero:</p>
<p><span class="math display">\[
0 = -2 \left( (y - \hat{y}(\alpha))^T W J + 2 h^T J^T W J \right)
\]</span></p>
<p>This simplifies to:</p>
<p><span class="math display">\[
h^T J^T W J = (y - \hat{y}(\alpha))^T W J
\]</span></p>
<p>Taking the transpose of both sides:</p>
<p><span class="math display">\[
\left[ h^T J^T W J \right]^T = \left[ (y - \hat{y}(\alpha))^T W J \right]^T
\]</span></p>
<p>Which simplifies further to:</p>
<p><span class="math display">\[
J^T W J h = J^T W (y - \hat{y}(\alpha))
\]</span></p>
<p>And finally, we can solve for the update direction <span class="math inline">\(h_{\text{gn}}\)</span>:</p>
<p><span class="math display">\[
h_{\text{gn}} = \left( J^T W J \right)^{-1} J^T W (y - \hat{y}(\alpha))
\]</span></p>
<p>This gives the update direction <span class="math inline">\(h_{\text{gn}}\)</span>, which tells us how to adjust the parameters to reduce the cost.</p>
<p>To update the parameters, we simply add this update direction to the current parameters:</p>
<p><span class="math display">\[
\theta_{\text{new}} = \theta_{\text{old}} + h_{\text{gn}}
\]</span></p>
<p>where <span class="math inline">\(h_{\text{gn}}\)</span> is the update direction computed as:</p>
<p><span class="math display">\[
h_{\text{gn}} = \left( J^T W J \right)^{-1} J^T W (y - \hat{y}(\alpha))
\]</span></p>
</div>
<div id="summary-and-comparison-to-gradient-descent" class="section level2 hasAnchor" number="2.4">
<h2><span class="header-section-number">2.4</span> Summary and Comparison to Gradient Descent<a href="the-gauss-newton-method.html#summary-and-comparison-to-gradient-descent" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>In this section, we used the <strong>Gauss-Newton method</strong> to update the parameters. Here’s how it works compared to <strong>gradient descent</strong>:</p>
<ol style="list-style-type: decimal">
<li><p><strong>Initial Guess</strong>: We start with an initial guess for the parameters, <span class="math inline">\(\theta_{\text{old}}\)</span>, which might be a random value but should be reasonable enough to start the optimization process.</p></li>
<li><p><strong>Compute the Gradient</strong>: The gradient (or in this case, the first derivative) of the cost function tells us the direction to move in. It gives us a sense of the slope of the “hill” we’re trying to descend.</p>
<p>For <strong>gradient descent</strong>, the update rule would be:</p>
<p><span class="math display">\[
\theta_{\text{new}} = \theta_{\text{old}} - \eta \nabla_{\theta} \chi^2(\theta)
\]</span></p>
<p>where <span class="math inline">\(\eta\)</span> is the learning rate (controlling the size of the update step) and <span class="math inline">\(\nabla_{\theta} \chi^2(\theta)\)</span> is the gradient of the cost function with respect to the parameters.</p>
<p>The <strong>Gauss-Newton method</strong> also uses a gradient-like update, but it takes into account <strong>second-order information</strong> (through the Jacobian matrix <span class="math inline">\(J\)</span>) to adjust the parameters more effectively, particularly when dealing with nonlinear models.</p>
<p>The Gauss-Newton update rule is:</p>
<p><span class="math display">\[
\theta_{\text{new}}  = \theta_{\text{old}} + \left( J^T W J \right)^{-1} J^T W (y - \hat{y}(\alpha))
\]</span></p></li>
<li><p><strong>Learning Rate</strong>: In <strong>gradient descent</strong>, we often introduce a <strong>learning rate</strong> <span class="math inline">\(\eta\)</span> to control the size of the updates. In <strong>Gauss-Newton</strong>, the update is determined by the matrix inversion, so we don’t need a separate learning rate, though it’s still possible to introduce one if necessary for stability.</p>
<p>For gradient descent:</p>
<p><span class="math display">\[
\theta_{\text{new}} = \theta_{\text{old}}  + \eta J^T W (y - \hat{y}(\alpha))
\]</span></p></li>
<li><p><strong>Second-order Information</strong>: The key difference is that the <strong>Gauss-Newton method</strong> uses second-order information (via the Jacobian matrix <span class="math inline">\(J\)</span> and the Hessian approximation <span class="math inline">\(J^T W J\)</span>) to better model how the cost function behaves near the current parameter values. This typically results in more efficient updates compared to <strong>gradient descent</strong>, especially for nonlinear models.</p></li>
</ol>
<hr />
<p>In summary, both methods aim to update the parameters <span class="math inline">\(\theta\)</span> to minimize the cost function <span class="math inline">\(\chi^2(\alpha)\)</span>, but the <strong>Gauss-Newton method</strong> incorporates second-order information (through the Jacobian and its inverse), which allows for more accurate and faster convergence, especially when dealing with nonlinear regression models.</p>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="the-gradient-descent-method.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="levenberg-marquardt-lm-algorithm.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
  "sharing": {
    "github": false,
    "facebook": true,
    "twitter": true,
    "linkedin": false,
    "weibo": false,
    "instapaper": false,
    "vk": false,
    "whatsapp": false,
    "all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
  },
  "fontsettings": {
    "theme": "white",
    "family": "sans",
    "size": 2
  },
  "edit": {
    "link": "https://github.com/USERNAME/REPO/edit/BRANCH/02-TheGaussNewton.Rmd",
    "text": "Edit"
  },
  "history": {
    "link": null,
    "text": null
  },
  "view": {
    "link": null,
    "text": null
  },
  "download": ["_main.HTML"],
  "search": {
    "engine": "fuse",
    "options": null
  },
  "toc": {
    "collapse": "subsection"
  }
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
